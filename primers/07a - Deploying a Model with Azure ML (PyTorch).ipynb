{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Deploying a Model as a Web Service\n\nIt's no good being able to create an accurate model if you can't deploy it for use in an application or service. In this notebook, we'll explore the *Azure Machine Learning Service* and the associated *Azure Machine Learning SDK*; which together enable you to train, deploy, and manage machine learning models at scale.\n\nTo use Azure Machine Learning, you're going to need an Azure subscription. If you don't already have one, you can sign up for a free trial at https://azure.microsoft.com/Account/Free.\n\n*Note: Azure Machine Learning provides a whole range of functionality to help you through the entire lifecycle of model development, training, evaluation, deployment, and management. We're going to focus on using it to deploy a trained model; but you can use it to do much, much more!*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## A Brief Introduction to Containers\nWhen you access a web site or a software service across the internet, you're probably dimly aware that somewhere, the code for the service is hosted on a *server*. We tend to think of servers as being physical computers, but in recent years there's been a growth in *virtualization* technologies so that a computer can be virtualized in software, and multiple *virtual machines* can be hosted on a single physical server.\n\nVirtual machines (VMs) are useful - in fact, the Azure Data Science Virtual Machine (DSVM) is a good example of a VM that enables you to provision a computer that contains the operating system (OS) and all the software applications you need to work with data and build machine learning models, and then you can delete the VM when you're finished with it so that you only pay for what you use - very cool!\n\nHowever, it seems wasteful to provision a complete virtual machine, including the full OS and applications, just to host a simple software service - especially if you need to support multiple services, each one consuming its own VM. *Containers* are an evolutionary step beyond VMs. They contain only the OS components that are required for the specific software service they need to host. This makes them very small compared to full VMs, which in turn means that they're portable, and quick to deploy and start up.\n\nContainers themselves are hosted in a container environment that provides all the common services and OS functionality they require. During development, this environment is often a locally installed system called *Docker*. When hosting a service in the cloud however, you can use container services such as *Azure Container Instances* (ACI), which is useful for lightweight hosting and testing of containerized services; or *Azure Kubernetes Services*, which provides a scalable and highly-available environment for managing clusters of containers, based on the industry standard *Kubernetes* container hosting platform.\n\nIn the rest of this notebook, we'll examine how you can use Azure Machine Learning Services to prepare a container image for your machine learning model, and deploy your model as a containerized web service that can be consumed by other applications that connect to it over an HTTP REST endpoint."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train a classification model\nLet's start by training a simple classification model so that we have something to deploy.\nIf you've completed the previous notebooks in this library, this should be pretty familiar - we're going to use PyTorch to train a simple shape classifier.\n\n> Note: If you **haven't** completed the previous notebooks, go and do it now - we'll wait!\n\nFirst, we'll install the latest version of PyTorch and import the libraries we'll use to train and test the model locally."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install PyTorch\n!pip install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n!pip install torchvision\n\n# Import PyTorch libraries\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nprint(\"Libraries imported - ready to use PyTorch\", torch.__version__)\n\n# Other libraries we'll use\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next we'll generate some images of geometric shapes with which to train and validate the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to create a random image (of a square, circle, or triangle)\ndef create_image (size, shape):\n    from random import randint\n    import numpy as np\n    from PIL import Image, ImageDraw\n    \n    xy1 = randint(10,40)\n    xy2 = randint(60,100)\n    col = (randint(0,200), randint(0,200), randint(0,200))\n\n    img = Image.new(\"RGB\", size, (255, 255, 255))\n    draw = ImageDraw.Draw(img)\n    \n    if shape == 'circle':\n        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n    elif shape == 'triangle':\n        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n    else: # square\n        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n    del draw\n    \n    return np.array(img)\n\n# function to create a dataset of images\ndef generate_image_data (classes, size, cases, img_dir):\n    import os, shutil\n    from PIL import Image\n    \n    if os.path.exists(img_dir):\n        replace_folder = input(\"Image folder already exists. Enter Y to replace it (this can take a while!). \\n\")\n        if replace_folder == \"Y\":\n            print(\"Deleting old images...\")\n            shutil.rmtree(img_dir)\n        else:\n            return # Quit - no need to replace existing images\n    os.makedirs(img_dir)\n    print(\"Generating new images...\")\n    i = 0\n    while(i < (cases - 1) / len(classes)):\n        if (i%25 == 0):\n            print(\"Progress:{:.0%}\".format((i*len(classes))/cases))\n        i += 1\n        for classname in classes:\n            img = Image.fromarray(create_image(size, classname))\n            saveFolder = os.path.join(img_dir,classname)\n            if not os.path.exists(saveFolder):\n                os.makedirs(saveFolder)\n            imgFileName = os.path.join(saveFolder, classname + str(i) + '.jpg')\n            try:\n                img.save(imgFileName)\n            except:\n                try:\n                    # Retry (resource constraints in Azure notebooks can cause occassional disk access errors)\n                    img.save(imgFileName)\n                except:\n                    # We gave it a shot - time to move on with our lives\n                    print(\"Error saving image\", imgFileName)\n                    \n# Function to ingest data using training and test loaders\ndef load_dataset(data_path):\n    # Load all of the images\n    transformation = transforms.Compose([\n        # transform to tensors\n        transforms.ToTensor(),\n        # Normalize the pixel values (in R, G, and B channels)\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    # Load all of the images, transforming them\n    full_dataset = torchvision.datasets.ImageFolder(\n        root=data_path,\n        transform=transformation\n    )\n    \n    \n    # Split into training (70% and testing (30%) datasets)\n    train_size = int(0.7 * len(full_dataset))\n    test_size = len(full_dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n    \n    # define a loader for the training data we can iterate through in 50-image batches\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=50,\n        num_workers=0,\n        shuffle=False\n    )\n    \n    # define a loader for the testing data we can iterate through in 50-image batches\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=50,\n        num_workers=0,\n        shuffle=False\n    )\n        \n    return train_loader, test_loader\n            \n# Our classes will be circles, squares, and triangles\nclassnames = ['circle', 'square', 'triangle']\n\n# All images will be 128x128 pixels\nimg_size = (128,128)\n\n# We'll store the images in a folder named 'shapes'\nfolder_name = 'shapes'\n\n# Generate 1200 random images.\ngenerate_image_data(classnames, img_size, 1200, folder_name)\n\n# Now load the images from the shapes folder\nprint(\"Loading image files in %s folder...\" % folder_name)\ndata_path = folder_name + '/'\n\n# Get the iterative dataloaders for test and training data\ntrain_loader, test_loader = load_dataset(data_path)\nprint(\"Data loaded, ready for model training.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we'll define and train the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a neural net class\nclass Net(nn.Module):\n    # Constructor\n    def __init__(self, num_classes=3):\n        super(Net, self).__init__()\n        \n        # Our images are RGB, so input channels = 3. We'll apply 12 filters in the first convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n        \n        # We'll apply max pooling with a kernel size of 2\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        \n        # A second convolutional layer takes 12 input channels, and generates 12 outputs\n        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n        \n        # A third convolutional layer takes 12 inputs and generates 24 outputs\n        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n        \n        # A drop layer deletes 20% of the features to help prevent overfitting\n        self.drop = nn.Dropout2d(p=0.2)\n        \n        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n        # So our feature tensors are now 32 x 32, and we've generated 24 of them\n        # We need to flatten these and feed them to a fully-connected layer\n        # to map them to  the probability for each class\n        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n\n    def forward(self, x):\n        # Use a relu activation function after convolution 1 and pool\n        x = F.relu(self.pool(self.conv1(x)))\n      \n        # Use a relu activation function after convolution 2 and pool\n        x = F.relu(self.pool(self.conv2(x)))\n        \n        # Select some features to drop after the 3rd convolution to prevent overfitting\n        x = F.relu(self.drop(self.conv3(x)))\n        \n        # Only drop the features if this is a training pass\n        x = F.dropout(x, training=self.training)\n        \n        # Flatten\n        x = x.view(-1, 32 * 32 * 24)\n        # Feed to fully-connected layer to predict class\n        x = self.fc(x)\n        # Return class probabilities via a log softmax function \n        return F.log_softmax(x, dim=1)\n    \ndef train(model, device, train_loader, optimizer, epoch):\n    # Set the model to training mode\n    model.train()\n    train_loss = 0\n    print(\"Epoch:\", epoch)\n    # Process the images in batches\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Use the CPU or GPU as appropriate\n        data, target = data.to(device), target.to(device)\n        \n        # Reset the optimizer\n        optimizer.zero_grad()\n        \n        # Push the data forward through the model layers\n        output = model(data)\n        \n        # Get the loss\n        loss = loss_criteria(output, target)\n        \n        # Keep a running total\n        train_loss += loss.item()\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n        # Print metrics for every 10 batches so we see some progress\n        if batch_idx % 10 == 0:\n            print('Training set [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n                batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n    # return average loss for the epoch\n    return train_loss / len(train_loader.dataset)\n            \n            \ndef test(model, device, test_loader):\n    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            \n            # Get the predicted classes for this batch\n            output = model(data)\n            \n            # Calculate the loss for this batch\n            test_loss += loss_criteria(output, target).item()\n            \n            # Calculate the accuracy for this batch\n            _, predicted = torch.max(output.data, 1)\n            correct += torch.sum(target==predicted).item()\n\n    # Calculate the average loss and total accuracy for this epoch\n    test_loss /= len(test_loader.dataset)\n    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    \n    # return average loss for the epoch\n    return test_loss\n    \n# Now Use the train and test functions to train and test the model    \n\ndevice = \"cpu\"\nif (torch.cuda.is_available()):\n    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n    device = \"cuda\"\nprint('Training on', device)\n\n# Create an instance of the model class and allocate it to the device\nmodel = Net(num_classes=len(classnames)).to(device)\n\n# Use an \"Adam\" optimizer to adjust weights\n# (see https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Specify the loss criteria\nloss_criteria = nn.CrossEntropyLoss()\n\n# Track metrics in these arrays\nepoch_nums = []\ntraining_loss = []\nvalidation_loss = []\n\n# Train over 5 epochs (in a real scenario, you'd likely use many more)\nepochs = 5\nfor epoch in range(1, epochs + 1):\n        train_loss = train(model, device, train_loader, optimizer, epoch)\n        test_loss = test(model, device, test_loader)\n        epoch_nums.append(epoch)\n        training_loss.append(train_loss)\n        validation_loss.append(test_loss)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Save and test the model locally\nOK, so we now have a trained shape classification model. Let's save it as a local file (well, local to the Azure Notebooks library anyway), and then load and test it; just to satisfy ourselves that it works:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to predict the class of an image\ndef predict_image(classifier, image_array):\n    import torch.utils.data as utils\n    import numpy as np\n    \n    # Set the classifer model to evaluation mode\n    classifier.eval()\n    \n    # Apply the same transformations as we did for the training images\n    transformation = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    # Preprocess the image\n    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n\n    # Turn the input into a Variable\n    input_features = Variable(image_tensor)\n\n    # Predict the class of the image\n    predictions = classifier(input_features)\n    \n    predicted_classes = []\n    for prediction in predictions.data.numpy():\n        class_idx = np.argmax(prediction)\n        predicted_classes.append(classnames[class_idx])\n    return np.array(predicted_classes)\n\n# Save the model weights\nmodel_file = 'shape-classifier.pth'\ntorch.save(model.state_dict(), model_file)\nprint(\"Model saved.\")\n\n# Delete the existing model variable\ndel model\n\n# Create a new model and load the weights\nmodel = Net()\nmodel.load_state_dict(torch.load(model_file))\nprint(\"New model created from saved weights\")\n\n\n# Now let's try it with a new image\nfrom random import randint\n\n# Create a random test image\nimg = create_image ((128,128), classnames[randint(0, len(classnames)-1)])\nplt.imshow(img)\n\n# Create an array of (1) images to match the expected input format\nimage_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2]).astype('float32')\n\npredicted_classes = predict_image(model, image_array)\nprint(predicted_classes[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It looks as though we have a working model. Now we're ready to use Azure Machine Learning to deploy it as a web service."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create an Azure Machine Learning workspace\n\nTo use Azure Machine Learning, you'll need to create a workspace in your Azure subscription.\n\nYour Azure subscription is identified by a subscription ID. To find this:\n1. Sign into the Azure portal at https://portal.azure.com.\n2. On the menu tab on the left, click &#128273; **Subscriptions**.\n3. View the list of your subscriptions and copy the ID for the subscription you want to use.\n4. Paste the subscription ID into the code below, and then run the cell to set the variable - you will use it later."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Replace YOUR_SUBSCRIPTION_ID in the following variable assignment:\nSUBSCRIPTION_ID = '9445c39e-3a88-47fb-bc9c-91ce9ee18873' #'YOUR_SUBSCRIPTION_ID'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To deploy the model file as a web service, we'll use the Azure Machine Learning SDK.\n\n> Note: the Azure Machine Learning SDK is installed by default in Azure Notebooks and the Azure Data Science Virtual Machine, but you may want to ensure that it's upgraded to the latest version. If you're using your own Python environment, you'll need to install it using the instructions in the [Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install azureml-sdk --upgrade\n\nimport azureml.core\nprint(azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To manage the deployment, we need an Azure ML workspace. Create one in your Azure subscription by running the following cell. If you're signed into Azure notebooks using the same credentials as your Azure subscription, you may be prompted to grant this notebooks project permission to use your Azure credentials. Otherwise, you'll be prompted to authenticate by entering a code at a given URL, so just click the link that's displayed and enter the specified code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\nws = Workspace.create(name='aml_workspace_pytorch', # or another name of your choosing\n                      subscription_id=SUBSCRIPTION_ID,\n                      resource_group='aml_resource_group', # or another name of your choosing\n                      create_resource_group=True,\n                      location='eastus2' # or other supported Azure region\n                     )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that you have a workspace, you can save the configuration so you can reconnect to it later."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n# Save the workspace config\nws.write_config()\n\n# Reconnect to the workspace (if you're not already signed in, you'll be prompted to authenticate with a code as before)\nws = Workspace.from_config()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a *scoring* file\nYour web service will need some Python code to load the input data, get the model, and generate and return a prediction. We'll save this code in a *scoring* file that will be deployed to the web service:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score_pytorch.py\n\n# create a scoring script that loads and infers from the model\nimport json\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom azureml.core.model import Model\n\ndef init():\n    try:\n        global model\n        MODEL_NAME = 'shape-classifier.pth'\n        # retieve the local path to the model using the model name\n        MODEL_PATH = Model.get_model_path(MODEL_NAME)\n        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        model = Net()\n        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n    except Exception as e:\n        result = str(e)\n        return json.dumps({\"error\": result})\n\n# REST API served by Azure ML supports json input\ndef run(json_data):\n    try:\n        data = np.array(json.loads(json_data)['data']).astype('float32')\n        predictions = predict_image(model, data)\n        return json.dumps(predictions.tolist())\n    except Exception as e:\n        result = str(e)\n        return json.dumps({\"error\": result})\n\n# Function to predict\ndef predict_image(classifier, image_array):\n    import torch\n    import torch.utils.data as utils\n    from torchvision import transforms\n    from torch.autograd import Variable\n    import numpy\n    \n    classifier.eval()\n    \n    transformation = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n\n    input_features = Variable(image_tensor)\n    predictions = classifier(input_features)\n    \n    classnames = ['circle', 'square', 'triangle']\n    \n    predicted_classes = []\n    for prediction in predictions.data.numpy():\n        class_idx = np.argmax(prediction)\n        predicted_classes.append(classnames[class_idx])\n    return np.array(predicted_classes)\n    \n# Define the Net class as used for training so we can load the trained weights\nclass Net(nn.Module):\n    def __init__(self, num_classes=3):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n        self.drop = nn.Dropout2d(p=0.2)\n        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.pool(self.conv1(x)))\n        x = F.relu(self.pool(self.conv2(x)))\n        x = F.relu(self.drop(self.conv3(x)))\n        x = F.dropout(x, training=self.training)\n        x = x.view(-1, 32 * 32 * 24)\n        x = self.fc(x)\n        return F.log_softmax(x, dim=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create an *environment* file\nThe web service will be hosted in a container, and the container will need to install any Python dependencies when it gets initialized. In this case, our scoring code requires the **torch** and **torchvision** Python libraries, so we'll create a .yml file that tells the container host to install these into the environment along with the default libraries used by Azure ML."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"pytorch\")\nmyenv.add_conda_package(\"torchvision\")\nmyenv.add_channel(\"pytorch\")\n\nenv_file = \"env_pytorch.yml\"\n\nwith open(env_file,\"w\") as f:\n    f.write(myenv.serialize_to_string())\nprint(\"Saved dependency info in\", env_file)\n\nwith open(env_file,\"r\") as f:\n    print(f.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Define a container image\nWe're going to deploy the web service as a container, so we need to define a container image that includes our scoring file and denvironment dependencies."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.image import ContainerImage\n\nimage_config = ContainerImage.image_configuration(execution_script = \"score_pytorch.py\",\n                                                  runtime = \"python\",\n                                                  conda_file = env_file,\n                                                  description = \"Container image for shape classification\",\n                                                  tags = {\"data\": \"shapes\", \"type\": \"classification\"}\n                                                 )\nprint(image_config.description)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Define the web service deployment configuration\nWe're going to deploy the containerized web service in the Azure Container Instance (ACI) service, so we need to specify the deployment configuration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naci_config = AciWebservice.deploy_configuration(cpu_cores = 1, \n                                               memory_gb = 1, \n                                               tags = {\"data\": \"shapes\", \"type\": \"classification\"},\n                                               description = 'shape classification service')\nprint(aci_config.description)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Deploy the web service \nNow we're ready to deploy. We'll deploy the container a service named **aci-pytorch-shape-svc**.\nThe deployment process includes the following steps:\n1. Register the model file in the Azure Machine Learning service (this also uploads the local model file to your Azure Machine Learning service so it can be deployed to a container)\n2. Create a container image for the web service, based on the configuration specified previously. This image will be used to instantiate the service.\n3. Create a service by deploying the container image (in this case to ACI - other hosts are available!)\n4. Verify the status of the deployed service.\n\n> For more information about Azure Container Instances, see https://azure.microsoft.com/en-us/services/container-instances.\n\nThis will take some time. When deployment has completed successfully, you'll see a status of **Healthy**."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import Webservice\n\nservice_name = 'aci-pytorch-shape-svc'\nservice = Webservice.deploy(deployment_config = aci_config,\n                                image_config = image_config,\n                                model_paths = [model_file],\n                                name = service_name,\n                                workspace = ws)\n\nservice.wait_for_deployment(show_output = True)\nprint(service.state)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use the web service\nWith the service deployed, now we can test it by using it to predict the shape of a new image."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nfrom random import randint\n\n# Create a random test image\nimg = create_image ((128,128), classnames[randint(0, len(classnames)-1)])\nplt.imshow(img)\n\n# Modify the image data to create an array of 1 image, matching the format of the training features\ninput_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n\n# Convert the array to JSON format\ninput_json = json.dumps({\"data\": input_array.tolist()})\n\n# Call the web service, passing the input data (the web service will also accept the data in binary format)\npredictions = service.run(input_data = input_json)\n\n# Get the predicted class - it'll be the first (and only) one.\nclassname = json.loads(predictions)[0]\nprint('The image is a', classname)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also send a batch of images to the service, and get back a prediction for each one."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nfrom random import randint\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Create three random test images\nfig = plt.figure(figsize=(6, 6))\nimages = []\ni = 0\nwhile(i < 3):  \n    # Create a new image\n    img = create_image((128,128), classnames[randint(0, len(classnames)-1)])\n    # Plot the image\n    a=fig.add_subplot(1,3,i + 1)\n    imgplot = plt.imshow(img)\n    # Add the image to an array to be submitted as a batch\n    images.append(img.tolist())\n    i += 1\n\n# Convert the array to JSON format\ninput_json = json.dumps({\"data\": images})\n\n# Call the web service, passing the input data\npredictions = service.run(input_data = input_json)\n\n# Get the predicted classes\nprint(json.loads(predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using the Web Service from Other Applications\nThe code above uses the Azure ML SDK to connect to the containerized web service and use it to generate predictions from your image classification model. In production, the model is likely to be consumed by business applications that make HTTP requests to the web service.\n\nLet's determine the URL to which these applications must submit their requests:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "endpoint = service.scoring_uri\nprint(endpoint)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we know the endpoint URI, an application can simply make an HTTP request, sending the image data in JSON (or binary) format, and receive back the predicted class(es)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nimport requests\nimport json\nfrom random import randint\n\n# Create a random test image\nimg = create_image ((128,128), classnames[randint(0, len(classnames)-1)])\nplt.imshow(img)\n\n# Create an array of (1) images to match the expected input format\nimage_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n\n# Convert the array to a serializable list in a JSON document\ninput_json = json.dumps({\"data\": image_array.tolist()})\n\n# Set the content type\nheaders = { 'Content-Type':'application/json' }\n\npredictions = requests.post(endpoint, input_json, headers = headers)\nprint(json.loads(predictions.content))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Deleting the Service\nWhen we're finished with the service, we can delete it to avoid incurring charges."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "service.delete()\nprint(\"Service deleted.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And if you're finished with the workspace, you can delete that too"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rg = ws.resource_group\nws.delete()\nprint(\"Workspace deleted. You should delete the '%s' resource group in your Azure subscription.\" % rg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Learn more\nTake a look at the Azure Machine Learning documentation at https://docs.microsoft.com/en-us/azure/machine-learning/service/."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}