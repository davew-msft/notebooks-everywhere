{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Convolutional Neural Networks\n\n\"Deep Learning\" is a general term that usually refers to the use of neural networks with multiple layers that synthesize the way the human brain learns and makes decisions. A convolutional neural network is a kind of neural network that extracts *features* from matrices of numeric values (often images) by convolving multiple filters over the matrix values to apply weights and identify patterns, such as edges, corners, and so on in an image. The numeric representations of these patterns are then passed to a fully-connected neural network layer to map the features to specific classes.\n\n## Basic Neural Network Recap\n\nYour brain works by connecting networks of neurons, each of which receives electrochemical stimuli from multiple inputs, which cause the neuron to fire under certain conditions. When a neuron fires, it creates an electrochemical charge that is passed as an input to one or more other neurons, creating a complex *feed-forward* network made up of layers of neurons that pass the signal on. An artificial neural network uses the same principles but the inputs are numeric values with associated *weights* that reflect their relative importance. The neuron take these input values and weights and applies them to an *activation function* that determines the ouput that the artificial neuron passes onto the next layer:\n\n<br/>\n<div align=\"center\" style='font-size:24px;'>&#8694;&#9711;&rarr;</div>\n\nAs the human brain learns from experience, the inputs to the neurons are strenghtened or weakened depending on their importance to the decisions that the brain needs to make in response to stimuli. Similarly, you train an artificial neural network using a supervised leaning technique in which a *loss function* is used to evaluate how well the multi-layered model detects known labels. You can then find the derivative of the loss function to determine whether the level of error (loss) is reduced by increasing or decreasing the weights associated with the inputs, and then apply *backpropagation* to adjust the weights and improve the model iteratively over multiple training *epochs*. The result of this training is a deep learning model that consists of:\n* An *input* layer to which the initial input variables are passed.\n* One or more *hidden* layers in which the weights optimized by training determine the signal that is fed forward through the network.\n* An *output* layer that presents the results.\n\n## Convolutional Neural Networks (CNNs)\nConvolutional Neural Networks, or *CNNs*, are a particular type of artificial neural network that works well with matrix inputs, such as images (which are fundamentally just multi-dimensional matrices of pixel intensity values). There are various kinds of layer in a CNN, but a common architecture is to build a sequence of *convolutional* layers that find patterns in indvidual areas of the input matrix and *pooling* layers that aggregate these patterns. Additionally, some layers may *drop* data (which helps avoid *overfitting* the model to the training data), and finally some layers will *flatten* the matrix data and a linear *dense*, or *fully connected* layer will perform classification and reshape the predictions to conform with the expected output format.\n\nConceptually, a Convolutional Neural Network for image classification is made up of multiple layers that extract features, such as edges, corners, etc; followed by one or more fully-connected layers to classify objects based on these features. You can visualize this like this:\n\n<table>\n    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Drop Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n    <tr><td colspan=5 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n</table>\n\n*Note: In Machine Learning, particularly \"deep learning\", matrices used in neural networks are often referred to as **tensors**. In a simplistic (which is to say, not strictly accurate) sense, a tensor is just a generalized term for a multi-dimensional matrix. In some deep learning frameworks, like PyTorch, a tensor is a specific type of data structure with properties and methods that support deep learning operations.*\n\n### Convolutional Layers\nConvolutional layers apply filters to a subregion of the input image, and *convolve* the filter across the image to extract features (such as edges, corners, etc.). For example, suppose the following matrix represents the pixels in a 6x6 image:\n\n$$\\begin{bmatrix}255 & 255 & 255 & 255 & 255 & 255\\\\255 & 255 & 0 & 0 & 255 & 255\\\\255 & 0 & 0 & 0 & 0 & 255\\\\255 & 0 & 0 & 0 & 0 & 255\\\\255 & 255 & 0 & 0 & 255 & 255\\\\255 & 255 & 255 & 255 & 255 & 255\\end{bmatrix}$$\n\nAnd let's suppose that a filter matrix is defined as a matrix of *weight* values like this:\n\n$$\\begin{bmatrix}0 & 1 & 0\\\\0 & 1 & 0\\\\0 & 1 & 0\\end{bmatrix}$$\n\nThe convolution layer applies the filter to the image matrix one \"patch\" at a time; so the first operation would apply to the <span style=\"color:red\">red</span> elements below:\n\n$$\\begin{bmatrix}\\color{red}{255} & \\color{red}{255} & \\color{red}{255} & 255 & 255 & 255\\\\\\color{red}{255} & \\color{red}{255} & \\color{red}{0} & 0 & 255 & 255\\\\\\color{red}{255} & \\color{red}{0} & \\color{red}{0} & 0 & 0 & 255\\\\255 & 0 & 0 & 0 & 0 & 255\\\\255 & 255 & 0 & 0 & 255 & 255\\\\255 & 255 & 255 & 255 & 255 & 255\\end{bmatrix}$$\n\nTo apply the filter, we multiply the patch area by the filter elementwise, and add the results:\n\n$$\\begin{bmatrix}255 & 255 & 255\\\\255 & 255 & 0\\\\255 & 0 & 0\\end{bmatrix} \\times \\begin{bmatrix}0 & 1 & 0\\\\0 & 1 & 0\\\\0 & 1 & 0\\end{bmatrix}= \\begin{bmatrix}(255 \\times 0) + (255 \\times 1) + (255 \\times 0) & +\\\\ (255 \\times 0) + (255 \\times 1) + (0 \\times 0) & + \\\\ (255 \\times 0) + (0 \\times 1) + (0 \\times 0)\\end{bmatrix}  = 510$$\n\nThis result is then used as the value for the first element of a feature map:\n\n$$\\begin{bmatrix}\\color{red}{510} & ? & ? & ?\\\\? & ? & ? & ?\\\\? & ? & ? & ?\\\\? & ? & ? & ?\\end{bmatrix}$$\n\nNext we move the patch along one pixel and apply the filter to the new patch area:\n\n$$\\begin{bmatrix}255 & \\color{red}{255} & \\color{red}{255} & \\color{red}{255} & 255 & 255\\\\255 & \\color{red}{255} & \\color{red}{0} & \\color{red}{0} & 255 & 255\\\\255 & \\color{red}{0} & \\color{red}{0} & \\color{red}{0} & 0 & 255\\\\255 & 0 & 0 & 0 & 0 & 255\\\\255 & 255 & 0 & 0 & 255 & 255\\\\255 & 255 & 255 & 255 & 255 & 255\\end{bmatrix}$$\n\n$$\\begin{bmatrix}255 & 255 & 255\\\\255 & 0 & 0\\\\0 & 0 & 0\\end{bmatrix} \\times \\begin{bmatrix}0 & 1 & 0\\\\0 & 1 & 0\\\\0 & 1 & 0\\end{bmatrix}= \\begin{bmatrix}(255 \\times 0) + (255 \\times 1) + (255 \\times 0) & +\\\\ (255 \\times 0) + (0 \\times 1) + (0 \\times 0) & + \\\\ (0 \\times 0) + (0 \\times 1) + (0 \\times 0)\\end{bmatrix}  = 255 $$\n\nSo can fill in that value on our feature map:\n$$\\begin{bmatrix}510 & \\color{red}{255} & ? & ?\\\\? & ? & ? & ?\\\\? & ? & ? & ?\\\\? & ? & ? & ?\\end{bmatrix}$$\n\nThen we just repeat the process, moving the patch across the entire image matrix until we have a completed feature map like this:\n\n$$\\begin{bmatrix}510 & 255 & 255 & 510\\\\255 & 0 & 0 & 255\\\\255 & 0 & 0 & 255\\\\510 & 255 & 255 & 510\\end{bmatrix}$$\n\nYou'll have noticed that as a result of convolving a patch across the original image, we've \"lost\" a 1-pixel strip around the edge. Typically, we apply a *padding* rule to keep the convolved image the same size as the original image, often by simply filling creating a 1-pixel wide edge with 0 values, like this:\n\n$$\\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0\\\\0 & 510 & 255 & 255 & 510 & 0\\\\0 & 255 & 0 & 0 & 255 & 0\\\\0 & 255 & 0 & 0 & 255 & 0\\\\0 & 510 & 255 & 255 & 510 & 0\\\\0 & 0 & 0 & 0 & 0 & 0\\end{bmatrix}$$\n\n### Pooling Layers\nAfter using one or more convolution layers to create a filter map, you can use a pooling layer to  reduce the number of dimensions in the matrix. A common technique is to use *MaxPooling*, in which a patch is applied to the matrix and the maximum value within the mask is retained while the others are discarded.\n\nFor example, we could apply a 2x2 patch to our feature map to extract the largest value in each 2x2 subarea:\n\n$$\\begin{bmatrix}\\color{blue}{0} & \\color{blue}{0} & \\color{green}{0} & \\color{green}{0} & \\color{red}{0} & \\color{red}{0}\\\\\\color{blue}{0} & \\color{blue}{510} & \\color{green}{255} & \\color{green}{255} & \\color{red}{510} & \\color{red}{0}\\\\\\color{magenta}{0} & \\color{magenta}{255} & \\color{orange}{0} & \\color{orange}{0} & \\color{cyan}{255} & \\color{cyan}{0}\\\\\\color{magenta}{0} & \\color{magenta}{255} & \\color{orange}{0} & \\color{orange}{0} & \\color{cyan}{255} & \\color{cyan}{0}\\\\\\color{brown}{0} & \\color{brown}{510} & 255 & 255 & \\color{yellow}{510} & \\color{yellow}{0}\\\\\\color{brown}{0} & \\color{brown}{0} & 0 & 0 & \\color{yellow}{0} & \\color{yellow}{0}\\end{bmatrix}\\Longrightarrow \\begin{bmatrix}\\color{blue}{510} & \\color{green}{255} & \\color{red}{510}\\\\\\color{magenta}{255} & \\color{orange}{0} & \\color{cyan}{255}\\\\\\color{brown}{510} & 255 & \\color{yellow}{510}\\end{bmatrix}$$\n\n### Activation Functions\nAfter each layer of filtering or pooling, it's common to apply a *rectified linear unit (ReLU)* function to the feature maps that have been produced. This has the effect of ensuring that all values in the feature maps are zero or higher.\n\n### Dropout Layers\nIn any machine learning training process, there is a danger of *overfitting* the model to the training data. In other words, you might end with a model that works extremely well with the data on which it was trained, but can't generalize effectively to classify new images. One way in which you can reduce the risk of overfitting is to randomly drop some of the feature maps.\n\n### Dense (Fully-Connected) Layers\nAfter the previous layers have created feature maps, a final linear *fully-connected* layer is used to generate class predictions - you can think of the fully-connected layer as being the endpoint of the classifier what determines which combination of features found in the previous layers \"adds up\" to a particular class. To create a fully-connected layer, the feature maps are flattened into a single 1-dimensional matrix and a function is applied to calculate the probability for each class that the model is designed to predict - usually this final function is a *Sigmoid* or *SoftMax* function that assigns a value between 0 and 1 to each class, with the total of these assignments adding to 1:\n\n$$\\begin{bmatrix}510 & 255 & 510\\\\255 & 0 & 255\\\\510 & 255 & 510\\end{bmatrix}\\begin{bmatrix}255 & 255 & 510\\\\255 & 0 & 255\\\\510 & 255 & 255\\end{bmatrix}...$$\n\n$$ \\Downarrow $$\n\n$$\\begin{bmatrix}510 & 255 & 510 & 255 & 0 & 255 & 510 & 255 & 510 & 255 & 255 & 510 & 255 & 0 & 255 & 510 & 255 & 255 ...\\end{bmatrix}$$\n\n$$ \\Downarrow $$\n\n$$\\begin{bmatrix}C_{1} & C_{2} & C_{3} \\\\ 0.15 & 0.8 & 0.05\\end{bmatrix}$$\n\n### Backpropagation\nWhen we train a CNN, we perform mulitple passes forward through the network of layers, and then use a *loss function* to measure the difference between the output values (which you may recall are probability predictions for each class) and the actual values for the known image classes used to train the model (in other words, 1 for the correct class and 0 for all the others). For example, in the example above the predicted probabilities are 0.15 for C<sub>1</sub>, 0.8 for C<sub>2</sub>, and 0.05 for C<sub>3</sub>. Let's suppose that the image in question is an example of C<sub>2</sub>, so the expected output is actually 0 for C<sub>1</sub>, 1 for C<sub>2</sub>, and 0 for C<sub>3</sub>. The error (or *loss*) represents how far from the expected values our results are.\n\nHaving calculated the loss, the training process uses a specified *optimizer* to calculate the derivitive of the loss function and determine how best to adjust the weights applied in the hidden layers to reduce the loss. We then go backwards through the network, adjusting the weights before the next forward pass.\n\n## Simulating Feature Extraction\nThe use of convolutional and pooling layers to extract features can seem a little abstract. It can be helpful to visualize the outputs of these layers to better understand how they help identify visual features in the images, reducing the size of the feature matrices that are passed to the fully-connected classification network layers at the end of the model.\n\nThe following code generates a sample image of a geometric shape, and then simulates the following feature extraction layers to simulate convolutional and pooling layers in a CNN:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom skimage.measure import block_reduce\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom PIL import ImageFilter\n%matplotlib inline\n\n\n# Function to create a random image (of a square, circle, or triangle)\ndef create_image (size, shape):\n    from random import randint\n    import numpy as np\n    from PIL import Image, ImageDraw\n    \n    xy1 = randint(10,40)\n    xy2 = randint(60,100)\n    col = (randint(0,200), randint(0,200), randint(0,200))\n\n    img = Image.new(\"RGB\", size, (255, 255, 255))\n    draw = ImageDraw.Draw(img)\n    \n    if shape == 'circle':\n        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n    elif shape == 'triangle':\n        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n    else: # square\n        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n    del draw\n    \n    return np.array(img)\n\n# Create a 128 x 128 pixel image (Let's use a square)\nimg = Image.fromarray(create_image((128,128), 'square'))\n\n# Now let's generate some feature extraction layers\nlayers = []\n\n# Define filter kernels - we'll use two filters\nkernel_size = (3,3) # kernels are 3 x 3\nkernel_1 = (1, 0, -100,\n            1, 0, -100,\n            1, 0, -100) # Mask for first filter\nkernel_2 = (-200, 0, 0,\n            0, -200, 0,\n            -0, 0, -200) # Mask for second filter\n\n# Define kernel size for pooling\npool_size = (2,2,1) # Pool filter is 2 x 2 pixels for each channel (so image size will half with each pool)\n\n# Convolutional layer\nthis_layer = []\n# Apply each filter to the original image - this generates a layer with two filtered images\nthis_layer.append(np.array(img.filter(ImageFilter.Kernel(kernel_size, kernel_1))))\nthis_layer.append(np.array(img.filter(ImageFilter.Kernel(kernel_size, kernel_2))))\nlayers.append(this_layer)\n             \n# Add a Pooling layer - pool each image in the previous layer by only using the maximum value in each 2x2 area\nthis_layer = []\nfor i in layers[len(layers)-1]:\n    # np.maximum implements a ReLU activation function so all pixel values are >=0\n    this_layer.append(np.maximum(block_reduce(i, pool_size, np.max), 0))\nlayers.append(this_layer)\n\n# Add a second convolutional layer - generates a new layer with 4 images (2 filters applied to 2 images in the previous layer)\nthis_layer = []\nfor i in layers[len(layers)-1]:\n    this_layer.append(np.array(Image.fromarray(i).filter(ImageFilter.Kernel(kernel_size, kernel_1))))\n    this_layer.append(np.array(Image.fromarray(i).filter(ImageFilter.Kernel(kernel_size, kernel_2))))\nlayers.append(this_layer)\n\n# Add a second Pooling layer - pool each image in the previous layer\nthis_layer = []\nfor i in layers[len(layers)-1]:\n    # np.maximum implements a ReLU activation function so all pixel values are >=0\n    this_layer.append(np.maximum(block_reduce(i, pool_size, np.max), 0))\nlayers.append(this_layer)\n\n# Set up a grid to plot the images in each layer\nfig = plt.figure(figsize=(16, 24))\nrows = len(layers) + 1\ncolumns = len(layers[len(layers)-1])\nrow = 0\nimage_no = 1\n\n# Plot the original image as layer 1\na=fig.add_subplot(rows,columns,image_no)\nimgplot = plt.imshow(img)\na.set_title('Original')\n\n# Plot the convolved and pooled layers\nfor layer in layers:\n    row += 1\n    image_no = row * columns\n    for image in layer:\n        image_no += 1\n        a=fig.add_subplot(rows,columns,image_no)\n        imgplot = plt.imshow(image)\n        a.set_title('Layer ' + str(row))\nplt.show() \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Look at the output, and note the following:\n\n* Each convolutional layer applies two filters, extracting features such as edges.\n* Each pooling layer reduces the overall size of the image by half, and has the effect of exaggerating the features by taking the maximum pixel value in each 2 x 2 square area.\n* A ReLU activation function is applied to each pooling layer, ensuring that any negative pixel values are set to 0.\n* The final layer represents the feature maps that have been extracted from the images - you should be able to see that the straight edges of the square have been detected.\n\nThe feature maps are just arrays of numbers, as shown by the following code:"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "for arr in layers[len(layers)-1]:\n    print(arr.shape, ':', arr.dtype)\n    print (arr)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These numeric feature values are then flattened and passed to the fully-connected layers to classify the image.\n\n## Building a CNN\nThere are several commonly used frameworks for creating CNNs, including *PyTorch*, *Tensorflow*, the *Microsoft Cognitive Toolkit (CNTK)*, and *Keras* (which is a high-level API that can use Tensorflow or CNTK as a back end). \n\n### A Simple Example\nThe example is a classification model that can classify images of geometric shapes.\n\nFirst, we'll use the function we created previously to generate some images for our classification model. Run the cell below to do that (note that it may take several minutes to run)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# function to create a dataset of images\ndef generate_image_data (classes, size, cases, img_dir):\n    import os, shutil\n    from PIL import Image\n    \n    if os.path.exists(img_dir):\n        replace_folder = input(\"Image folder already exists. Enter Y to replace it (this can take a while!). \\n\")\n        if replace_folder == \"Y\":\n            print(\"Deleting old images...\")\n            shutil.rmtree(img_dir)\n        else:\n            return # Quit - no need to replace existing images\n    os.makedirs(img_dir)\n    print(\"Generating new images...\")\n    i = 0\n    while(i < (cases - 1) / len(classes)):\n        if (i%25 == 0):\n            print(\"Progress:{:.0%}\".format((i*len(classes))/cases))\n        i += 1\n        for classname in classes:\n            img = Image.fromarray(create_image(size, classname))\n            saveFolder = os.path.join(img_dir,classname)\n            if not os.path.exists(saveFolder):\n                os.makedirs(saveFolder)\n            imgFileName = os.path.join(saveFolder, classname + str(i) + '.jpg')\n            try:\n                img.save(imgFileName)\n            except:\n                try:\n                    # Retry (resource constraints in Azure notebooks can cause occassional disk access errors)\n                    img.save(imgFileName)\n                except:\n                    # We gave it a shot - time to move on with our lives\n                    print(\"Error saving image\", imgFileName)\n            \n# Our classes will be circles, squares, and triangles\nclassnames = ['circle', 'square', 'triangle']\n\n# All images will be 128x128 pixels\nimg_size = (128,128)\n\n# We'll store the images in a folder named 'shapes'\nfolder_name = 'shapes'\n\n# Generate 1200 random images.\ngenerate_image_data(classnames, img_size, 1200, folder_name)\n\nprint(\"Image files ready in %s folder!\" % folder_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Setting up the Frameworks\nNow that we have our data, we're ready to build a CNN. The first step is to install and configure the framework we want to use.\n\nWe're going to use the Keras machine learning framewor with the default TensorFlow back-end.\n\n> **Note**: To install Keras on your own system, consult the Keras installation documentation at https://keras.io/#installation."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade keras\n\nimport keras\nfrom keras import backend as K\n\nprint('Keras version:',keras.__version__)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Preparing the Data\nBefore we can train the model, we need to prepare the data. We'll divide the feature values by 255 to normalize them as floating point values between 0 and 1, and we'll split the data so that we can use 70% of it to train the model, and hold back 30% to validate it. When loading the data, the data generator will assing \"hot-encoded\" numeric labels to indicate which class each image belongs to based on the subfolders in which the data is stored. In this case, there are three subfolders - *circle*, *square*, and *triangle*, so the labels will consist of three *0* or *1* values indicating which of these classes is associated with the image - for example the label [0 1 0] indicates that the image belongs to the second class (*square*)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.image import ImageDataGenerator\n\ndata_folder = 'shapes'\nimg_size = (128, 128)\nbatch_size = 30\n\nprint(\"Getting Data...\")\ndatagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n                             validation_split=0.3) # hold back 30% of the images for validation\n\nprint(\"Preparing training dataset...\")\ntrain_generator = datagen.flow_from_directory(\n    data_folder,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training') # set as training data\n\nprint(\"Preparing validation dataset...\")\nvalidation_generator = datagen.flow_from_directory(\n    data_folder,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation') # set as validation data\n\nclassnames = list(train_generator.class_indices.keys())\nprint(\"class names: \", classnames)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Defining the CNN\nNow we're ready to train our model. This involves defining the layers for our CNN, and compiling them for multi-class classification."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define a CNN classifier network\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import optimizers\n\n# Define the model as a sequence of layers\nmodel = Sequential()\n\n# The input layer accepts an image and applies a convolution that uses 32 6x6 filters and a rectified linear unit activation function\nmodel.add(Conv2D(32, (6, 6), input_shape=train_generator.image_shape, activation='relu'))\n\n# Next we;ll add a max pooling layer with a 2x2 patch\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n# We can add as many layers as we think necessary - here we'll add another convolution, max pooling, and dropout layer\nmodel.add(Conv2D(32, (6, 6), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# And another set\nmodel.add(Conv2D(32, (6, 6), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# A dropout layer randomly drops some nodes to reduce inter-dependencies (which can cause over-fitting)\nmodel.add(Dropout(0.2))\n\n# Now we'll flatten the feature maps and generate an output layer with a predicted probability for each class\nmodel.add(Flatten())\nmodel.add(Dense(train_generator.num_classes, activation='sigmoid'))\n\n# We'll use the ADAM optimizer\n# For information about the optimizers available in PyTorch, see https://pytorch.org/docs/stable/optim.html#algorithms\nopt = optimizers.Adam(lr=0.001)\n\n# With the layers defined, we can now compile the model for categorical (multi-class) classification\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nprint(model.summary())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Training the Model\nWith the layers of the CNN defined, we're ready to train the model using our image data. In the example below, we use 3 iterations (*epochs*) to train the model in 30-image batches, holding back 30% of the data for validation. After each epoch, the loss function measures the error (*loss*) in the model and adjusts the weights (which were randomly generated for the first iteration) to try to improve accuracy. \n\n> **Note**: We're only using 3 epochs to reduce the training time for this simple example. A real-world CNN is usually trained over more epochs than this. CNN model training is processor-intensive, so it's recommended to perform this on a system that can leverage GPUs (such as the Data Science Virtual Machine in Azure) to reduce training time. This will take a while to complete in Azure Notebooks (in which GPUs are not available) - status will be displayed as the training progresses. Feel free to go get some coffee!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train the model over 3 epochs using the validation holdout dataset for validation\nnum_epochs = 3\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = num_epochs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### View the Loss History\nWe tracked average training and validation loss history for each epoch. We can plot these to verify that loss reduced as the model was trained, and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nepoch_nums = range(1,num_epochs+1)\ntraining_loss = history.history[\"loss\"]\nvalidation_loss = history.history[\"val_loss\"]\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Evaluate Model Performance\nWe can see the final accuracy based on the test data, but typically we'll want to explore performance metrics in a little more depth. Let's plot a confusion matrix to see how well the model is predicting each class."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Keras doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint(\"Generating predictions from validation data...\")\n# Get the image and label arrays for the first batch of validation data\nx_test = validation_generator[0][0]\ny_test = validation_generator[0][1]\n\n# Use the moedl to predict the class\nclass_probabilities = model.predict(x_test)\n\n# The model returns a probability value for each class\n# The one with the highest probability is the predicted class\npredictions = np.argmax(class_probabilities, axis=1)\n\n# The actual labels are hot encoded (e.g. [0 1 0], so get the one with the value 1\ntrue_labels = np.argmax(y_test, axis=1)\n\n# Plot the confusion matrix\ncm = confusion_matrix(true_labels, predictions)\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\ntick_marks = np.arange(len(classnames))\nplt.xticks(tick_marks, classnames, rotation=85)\nplt.yticks(tick_marks, classnames)\nplt.xlabel(\"Predicted Shape\")\nplt.ylabel(\"True Shape\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Save the Model\nNow that we have trained the model, we can save it with the trained weights. Then later, we can reload it and use it to predict classes from new images."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\n\nmodelFileName = 'shape-classifier.h5'\n\nmodel.save(modelFileName) # saves the trained model\nprint(\"Model saved.\")\n\ndel model  # deletes the existing model variable",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using the Trained Model\nNow that we've trained the model, we can use it to predict the class of a new image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to predict the class of an image\ndef predict_image(classifier, image_array):\n    import numpy as np\n    \n    # We need to format the input to match the training data\n    # The generator loaded the values as floating point numbers\n    # and normalized the pixel values, so...\n    imgfeatures = image_array.astype('float32')\n    imgfeatures /= 255\n    \n    # These are the classes our model can predict\n    classes = ['circle', 'square', 'triangle']\n    \n    # Predict the class of each input image\n    predictions = classifier.predict(imgfeatures)\n    \n    predicted_classes = []\n    for prediction in predictions:\n        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n        # So get the index of the highest probability\n        class_idx = np.argmax(prediction)\n        # And append the corresponding class name to the results\n        predicted_classes.append(classes[int(class_idx)])\n    # Return the predictions as a JSON\n    return predicted_classes\n\n\nfrom random import randint\nimport numpy as np\n%matplotlib inline\n\n# load the saved model\nmodel = load_model(modelFileName) \n\n# Create a random test image\nimg = create_image ((128,128), classnames[randint(0, len(classnames)-1)])\nplt.imshow(img)\n\n# Create an array of (1) images to match the expected input format\nimg_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n\n# get the predicted clases\npredicted_classes = predict_image(model, img_array)\n\n# Display the prediction for the first image (we only submitted one!)\nprint(predicted_classes[0])",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}