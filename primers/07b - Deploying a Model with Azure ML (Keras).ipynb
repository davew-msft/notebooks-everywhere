{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Deploying a Model as a Web Service\n\nIt's no good being able to create an accurate model if you can't deploy it for use in an application or service. In this notebook, we'll explore the *Azure Machine Learning Service* and the associated *Azure Machine Learning SDK*; which together enable you to train, deploy, and manage machine learning models at scale.\n\nTo use Azure Machine Learning, you're going to need an Azure subscription. If you don't already have one, you can sign up for a free trial at https://azure.microsoft.com/Account/Free.\n\n*Note: Azure Machine Learning provides a whole range of functionality to help you through the entire lifecycle of model development, training, evaluation, deployment, and management. We're going to focus on using it to deploy a trained model; but you can use it to do much, much more!*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## A Brief Introduction to Containers\nWhen you access a web site or a software service across the internet, you're probably dimly aware that somewhere, the code for the service is hosted on a *server*. We tend to think of servers as being physical computers, but in recent years there's been a growth in *virtualization* technologies so that a computer can be virtualized in software, and multiple *virtual machines* can be hosted on a single physical server.\n\nVirtual machines (VMs) are useful - in fact, the Azure Data Science Virtual Machine (DSVM) is a good example of a VM that enables you to provision a computer that contains the operating system (OS) and all the software applications you need to work with data and build machine learning models, and then you can delete the VM when you're finished with it so that you only pay for what you use - very cool!\n\nHowever, it seems wasteful to provision a complete virtual machine, including the full OS and applications, just to host a simple software service - especially if you need to support multiple services, each one consuming its own VM. *Containers* are an evolutionary step beyond VMs. They contain only the OS components that are required for the specific software service they need to host. This makes them very small compared to full VMs, which in turn means that they're portable, and quick to deploy and start up.\n\nContainers themselves are hosted in a container environment that provides all the common services and OS functionality they require. During development, this environment is often a locally installed system called *Docker*. When hosting a service in the cloud however, you can use container services such as *Azure Container Instances* (ACI), which is useful for lightweight hosting and testing of containerized services; or *Azure Kubernetes Services*, which provides a scalable and highly-available environment for managing clusters of containers, based on the industry standard *Kubernetes* container hosting platform.\n\nIn the rest of this notebook, we'll examine how you can use Azure Machine Learning Services to prepare a container image for your machine learning model, and deploy your model as a containerized web service that can be consumed by other applications that connect to it over an HTTP REST endpoint."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train a classification model\nLet's start by training a simple classification model so that we have something to deploy.\nIf you've completed the previous notebooks in this library, this should be pretty familiar - we're going to use Keras to train a simple shape classifier.\n\n> Note: If you **haven't** completed the previous notebooks, go and do it now - we'll wait!*\n\nFirst, we'll import the libraries we're going to need to train and validate a model using Keras."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade keras\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.utils import np_utils\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\n%matplotlib inline\n\nprint(\"Ready to train a model using Keras %s\" % keras.__version__)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next we'll generate some images of geometric shapes with which to train and validate the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to create a random image (of a square, circle, or triangle)\ndef create_image (size, shape):\n    from random import randint\n    import numpy as np\n    from PIL import Image, ImageDraw\n    \n    xy1 = randint(10,40)\n    xy2 = randint(60,100)\n    col = (randint(0,200), randint(0,200), randint(0,200))\n\n    img = Image.new(\"RGB\", size, (255, 255, 255))\n    draw = ImageDraw.Draw(img)\n    \n    if shape == 'circle':\n        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n    elif shape == 'triangle':\n        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n    else: # square\n        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n    del draw\n    \n    return np.array(img)\n\n# function to create a dataset of images\ndef generate_image_data (classes, size, cases, img_dir):\n    import os, shutil\n    from PIL import Image\n    \n    if os.path.exists(img_dir):\n        replace_folder = input(\"Image folder already exists. Enter Y to replace it (this can take a while!). \\n\")\n        if replace_folder == \"Y\":\n            print(\"Deleting old images...\")\n            shutil.rmtree(img_dir)\n        else:\n            return # Quit - no need to replace existing images\n    os.makedirs(img_dir)\n    print(\"Generating new images...\")\n    i = 0\n    while(i < (cases - 1) / len(classes)):\n        if (i%25 == 0):\n            print(\"Progress:{:.0%}\".format((i*len(classes))/cases))\n        i += 1\n        for classname in classes:\n            img = Image.fromarray(create_image(size, classname))\n            saveFolder = os.path.join(img_dir,classname)\n            if not os.path.exists(saveFolder):\n                os.makedirs(saveFolder)\n            imgFileName = os.path.join(saveFolder, classname + str(i) + '.jpg')\n            try:\n                img.save(imgFileName)\n            except:\n                try:\n                    # Retry (resource constraints in Azure notebooks can cause occassional disk access errors)\n                    img.save(imgFileName)\n                except:\n                    # We gave it a shot - time to move on with our lives\n                    print(\"Error saving image\", imgFileName)\n                    \n# Our classes will be circles, squares, and triangles\nclassnames = ['circle', 'square', 'triangle']\n\n# All images will be 128x128 pixels\nimg_size = (128,128)\n\n# We'll store the images in a folder named 'shapes'\ndata_folder = 'shapes'\n\n# Generate 1200 random images.\ngenerate_image_data(classnames, img_size, 1200, data_folder)\n\nbatch_size = 30\n\nprint(\"Loading Data from ...%s folder!\" % data_folder)\ndatagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n                             validation_split=0.3) # hold back 30% of the images for validation\n\nprint(\"Preparing training dataset...\")\ntrain_generator = datagen.flow_from_directory(\n    data_folder,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training') # set as training data\n\nprint(\"Preparing validation dataset...\")\nvalidation_generator = datagen.flow_from_directory(\n    data_folder,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation') # set as validation data\n\nprint(\"Data loaded, ready for model training.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we'll define and train the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define a CNN classifier network\nmodel = Sequential()\n\n# The input layer accepts an image and applies a convolution that uses 32 6x6 filters and a rectified linear unit activation function\nmodel.add(Conv2D(32, (6, 6), input_shape=train_generator.image_shape, activation='relu'))\n\n# Next we'll add a max pooling layer with a 2x2 patch\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n# We can add as many layers as we think necessary - here we'll add another convolution, max pooling, and dropout layer\nmodel.add(Conv2D(32, (6, 6), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# And another set\nmodel.add(Conv2D(32, (6, 6), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# A dropout layer randomly drops some nodes to reduce inter-dependencies (which can cause over-fitting)\nmodel.add(Dropout(0.2))\n\n# Now we'll flatten the feature maps and generate an output layer with a predicted probability for each class\nmodel.add(Flatten())\nmodel.add(Dense(len(classnames), activation='sigmoid'))\n\n# With the layers defined, we can now compile the model for categorical (multi-class) classification\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# Train the model over 3 epochs using 30-image batches and using the validation holdout dataset for validation\nnum_epochs = 3\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = num_epochs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Save and test the model locally\nOK, so we now have a trained shape classification model. Let's save it as a local file (well, local to the Azure Notebooks library anyway), and then load and test it; just to satisfy ourselves that it works:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def predict_image(classifier, image_array):\n    import numpy as np\n    \n    # We need to format the input to match the training data\n    # The generator loaded the values as floating point numbers\n    # and normalized the pixel values, so...\n    imgfeatures = image_array.astype('float32')\n    imgfeatures /= 255\n    \n    # These are the classes our model can predict\n    classnames = ['circle', 'square', 'triangle']\n    \n    # Predict the class of each input image\n    predictions = classifier.predict(imgfeatures)\n    \n    predicted_classes = []\n    for prediction in predictions:\n        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n        # So get the index of the highest probability\n        class_idx = np.argmax(prediction)\n        # And append the corresponding class name to the results\n        predicted_classes.append(classnames[int(class_idx)])\n    # Return the predictions as a JSON\n    return predicted_classes\n\n\nfrom keras.models import load_model\nfrom random import randint\n\nmodelFileName = 'shape-classifier.h5'\n\nmodel.save(modelFileName) # saves the trained model\ndel model  # deletes the existing model variable\n\nmodel = load_model(modelFileName) # loads the saved model\n\n# Create a random test image\nimg = create_image ((128,128), classnames[randint(0, len(classnames)-1)])\nplt.imshow(img)\n\n# Create an array of (1) images to match the expected input format\nimg_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n\n# get the predicted clases\npredicted_classes = predict_image(model, img_array)\n\n# Display the prediction for the first image (we only submitted one!)\nprint(predicted_classes[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It looks as though we have a working model. Now we're ready to use Azure Machine Learning to deploy it as a web service."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create an Azure Machine Learning workspace\n\nTo use Azure Machine Learning, you'll need to create a workspace in your Azure subscription.\n\nYour Azure subscription is identified by a subscription ID. To find this:\n1. Sign into the Azure portal at https://portal.azure.com.\n2. On the menu tab on the left, click &#128273; **Subscriptions**.\n3. View the list of your subscriptions and copy the ID for the subscription you want to use.\n4. Paste the subscription ID into the code below, and then run the cell to set the variable - you will use it later."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Replace YOUR_SUBSCRIPTION_ID in the following variable assignment:\nSUBSCRIPTION_ID = 'YOUR_SUBSCRIPTION_ID'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To deploy the model file as a web service, we'll use the Azure Machine Learning SDK.\n\n> Note: the Azure Machine Learning SDK is installed by default in Azure Notebooks and the Azure Data Science Virtual Machine, but you may want to ensure that it's upgraded to the latest version. If you're using your own Python environment, you'll need to install it using the instructions in the [Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install azureml-sdk --upgrade\n\nimport azureml.core\nprint(azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To manage the deployment, we need an Azure ML workspace. Create one in your Azure subscription by running the following cell. If you're signed into Azure notebooks using the same credentials as your Azure subscription, you may be prompted to grant this notebooks project permission to use your Azure credentials. Otherwise, you'll be prompted to authenticate by entering a code at a given URL, so just click the link that's displayed and enter the specified code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\nws = Workspace.create(name='my_aml_workspace_keras', # or another name of your choosing\n                      subscription_id=SUBSCRIPTION_ID,\n                      resource_group='aml_resource_group', # or another name of your choosing\n                      create_resource_group=True,\n                      location='eastus2' # or other supported Azure region\n                     )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that you have a workspace, you can save the configuration so you can reconnect to it later."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n# Save the workspace config\nws.write_config()\n\n# Reconnect to the workspace (if you're not already signed in, you'll be prompted to authenticate with a code as before)\nws = Workspace.from_config()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a *scoring* file\nYour web service will need some Python code to load the input data, get the model, and generate and return a prediction. We'll save this code in a *scoring* file that will be deployed to the web service:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score_keras.py\n# scoring script used by service to load model and generate prediction\nimport json\nimport numpy as np\nimport os\nfrom keras.models import load_model\nfrom azureml.core.model import Model\n\n# Called when the service is loaded\ndef init():\n    global model\n    # Get the path to the deployed model file and load it\n    model_path = Model.get_model_path('shape-classifier.h5')\n    model = load_model(model_path)\n\n# Called when a request is received\ndef run(raw_data):\n    # Get the input data - the image(s) to be classified.\n    data = np.array(json.loads(raw_data)['data'])\n    \n    # Pre-process the images\n    imgfeatures = data.astype('float32')\n    imgfeatures /= 255\n\n    # Get a prediction from the model\n    predictions = model.predict(imgfeatures)\n    # get thge classname for the highest probability prediction for each input\n    classnames = ['circle', 'square', 'triangle']\n    predicted_classes = []\n    for prediction in predictions:\n        class_idx = np.argmax(prediction)\n        predicted_classes.append(classnames[int(class_idx)])\n    # Return the predictions as a JSON\n    return json.dumps(predicted_classes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create an *environment* file\nThe web service will be hosted in a container, and the container will need to install any required Python dependencies when it gets initialized. In this case, our scoring code requires **scikit-learn**, so we'll create a .yml file that tells the container host to install this into the environment."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"keras\")\n\nenv_file = \"env_keras.yml\"\n\nwith open(env_file,\"w\") as f:\n    f.write(myenv.serialize_to_string())\nprint(\"Saved dependency info in\", env_file)\n\nwith open(env_file,\"r\") as f:\n    print(f.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Define a container image\nWe're going to deploy the web service as a container, so we need to define a container image that includes our scoring file and denvironment dependencies."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.image import ContainerImage\n\nimage_config = ContainerImage.image_configuration(execution_script = \"score_keras.py\",\n                                                  runtime = \"python\",\n                                                  conda_file = \"env_keras.yml\",\n                                                  description = \"Container image for shape classification\",\n                                                  tags = {\"data\": \"shapes\", \"type\": \"classification\"}\n                                                 )\nprint(image_config.description)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Define the web service deployment configuration\nWe're going to deploy the containerized web service in the Azure Container Instance (ACI) service, so we need to specify the deployment configuration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naci_config = AciWebservice.deploy_configuration(cpu_cores = 1, \n                                               memory_gb = 1, \n                                               tags = {\"data\": \"shapes\", \"type\": \"classification\"},\n                                               description = 'shape classification service')\nprint(aci_config.description)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Deploy the web service \nNow we're ready to deploy. We'll deploy the container a service named **aci-shape-svc**.\nThe deployment process includes the following steps:\n1. Register the model file in the Azure Machine Learning service (this also uploads the local model file to your Azure Machine Learning service so it can be deployed to a container)\n2. Create a container image for the web service, based on the configuration specified previously. This image will be used to instantiate the service.\n3. Create a service by deploying the container image (in this case to ACI - other hosts are available!)\n4. Verify the status of the deployed service.\n\nThis will take some time. When deployment has completed successfully, you'll see a status of **Healthy**."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import Webservice\n\nservice_name = 'aci-shape-keras-svc'\nservice = Webservice.deploy(deployment_config = aci_config,\n                                image_config = image_config,\n                                model_paths = ['shape-classifier.h5'],\n                                name = service_name,\n                                workspace = ws)\n\nservice.wait_for_deployment(show_output = True)\nprint(service.state)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use the web service\nWith the service deployed, now we can test it by using it to predict the shape of a new image."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nfrom random import randint\n\n# Create a random test image\nimg = create_image ((128,128), classnames[randint(0, len(classnames)-1)])\nplt.imshow(img)\n\n# Modify the image data to create an array of 1 image, matching the format of the training features\ninput_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n\n# Convert the array to JSON format\ninput_json = json.dumps({\"data\": input_array.tolist()})\n\n# Call the web service, passing the input data (the web service will also accept the data in binary format)\npredictions = service.run(input_data = input_json)\n\n# Get the predicted class - it'll be the first (and only) one.\nclassname = json.loads(predictions)[0]\nprint('The image is a', classname)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also send a batch of images to the service, and get back a prediction for each one."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nfrom random import randint\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Create three random test images\nfig = plt.figure(figsize=(6, 6))\nimages = []\ni = 0\nwhile(i < 3):  \n    # Create a new image\n    img = create_image((128,128), classnames[randint(0, len(classnames)-1)])\n    # Plot the image\n    a=fig.add_subplot(1,3,i + 1)\n    imgplot = plt.imshow(img)\n    # Add the image to an array to be submitted as a batch\n    images.append(img.tolist())\n    i += 1\n\n# Convert the array to JSON format\ninput_json = json.dumps({\"data\": images})\n\n# Call the web service, passing the input data\npredictions = service.run(input_data = input_json)\n\n# Get the predicted classes\nprint(json.loads(predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using the Web Service from Other Applications\nThe code above uses the Azure ML SDK to connect to the containerized web service and use it to generate predictions from your image classification model. In production, the model is likely to be consumed by business applications that make HTTP requests to the web service.\n\nLet's determine the URL to which these applications must submit their requests:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "endpoint = service.scoring_uri\nprint(endpoint)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we know the endpoint URI, an application can simply make an HTTP request, sending the image data in JSON (or binary) format, and receive back the predicted class(es)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nimport requests\nimport json\nfrom random import randint\n\n# Create a random test image\nimg = create_image ((128,128), classnames[randint(0, len(classnames)-1)])\nplt.imshow(img)\n\n# Create an array of (1) images to match the expected input format\nimage_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n\n# Convert the array to a serializable list in a JSON document\ninput_json = json.dumps({\"data\": image_array.tolist()})\n\n# Set the content type\nheaders = { 'Content-Type':'application/json' }\n\npredictions = requests.post(endpoint, input_json, headers = headers)\nprint(json.loads(predictions.content))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Deleting the Service\n\nWhen we're finished with the service, we can delete it to avoid incurring charges."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "service.delete()\nprint(\"Service deleted.\")",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "No service with name aci-shape-keras-svc found to delete.\nService deleted.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And if you're finished with the workspace, you can delete that too"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rg = ws.resource_group\nws.delete()\nprint(\"Workspace deleted. You should delete the ''%s' resource group in your Azure subscription.\" % rg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Learn more\nTake a look at the Azure Machine Learning documentation at https://docs.microsoft.com/en-us/azure/machine-learning/service/."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}