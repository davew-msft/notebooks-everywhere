{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Object Detection with the Custom Vision Service\n\nIn the world of computer vision, *object detection* is the next step from *image classification*. In image classification, your model classifies images based on their content. In object detection, the model determines the location of one or more objects in an image - typically predicting the coordinates of a *bounding box* that surrounds each object in the image.\n\nIn this notebook, we'll build a model that can detect apples and carrots.\n\n*Some of the images used in the lab are sourced from the free image library at <a href='http://www.pachd.com' target='_blank'>www.pachd.com</a>*\n\n## Frameworks for Object Detection\n\nThere are many ways to create an object detection model, including machine learning frameworks like TensorFlow and the Microsoft Cognitive Toolkit (CNTK). The Microsoft *Custom Vision* cognitive service also provides an API that you can use for object detection; and that's what we'll use in this notebook.\n\nFirst, we need to install the SDK:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install the Custom Vision SDK\n! pip install azure-cognitiveservices-vision-customvision",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Sign up for a Custom Vision service account\nNow you're ready to use the Custom Vision service. You'll need to sign up for an account and get your unique training and prediction keys so you can access it:\n1. If you don't already have a Microsoft Azure subscription, sign up at https://azure.microsoft.com/en-us/. \n2. Go to https://customvision.ai/ and sign in using your Microsoft account. Then create a new custom vision service account in your Azure subscription.\n3. Click the *Settings* (&#9881;) icon at the top right to view *training* and  *prediction* key's endpoints, and resource Ids. Then assign the appropriate values to the variables below and run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "TRAINING_KEY = 'YOUR_TRAINING_KEY'\nPREDICTION_KEY = 'YOUR_PREDICTION_KEY'\nENDPOINT='https://YOUR_REGION.api.cognitive.microsoft.com' # Use just the base URL - https://<region>.api.cognitive.microsoft.com\nPREDICTION_RESOURCE_ID=\"/subscriptions/YOUR_SUBSCRIPTION_ID/resourceGroups/YOUR_RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/YOUR_ACCOUNT_Prediction\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create an Object Detection project\nNext, you need to create a new Cutom Vision project for Object Detection."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateEntry, Region\n\ntrainer = CustomVisionTrainingClient(TRAINING_KEY, endpoint=ENDPOINT)\n\n# Find the object detection domain\nobj_detection_domain = next(domain for domain in trainer.get_domains() if domain.type == \"ObjectDetection\")\n\n# Create a new project\nprint (\"Creating project...\")\nproject = trainer.create_project(\"Produce Detection\", domain_id=obj_detection_domain.id)\nprint(\"Created project!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create tags\nJust as with image classification, we need to define the tags that identify the different types of object class our model will detect - in this case, apples and carrots."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Creating tags...\")\napple_tag = trainer.create_tag(project.id, \"Apple\")\ncarrot_tag = trainer.create_tag(project.id, \"Carrot\")\nprint('Created tags')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Define object regions in training images\nThe key difference between object detection and image classification is that object detection predicts not just the *class* (tag) of the objects in an image, but also a bounding box that encloses their location. To accomplish this, you need to train the model using not just images but also the coordinates of the bounding boxes in the training images.\n\nThere are various tools you can use to tag the bounding boxes in images, including the <a href='https://github.com/Microsoft/VoTT' target='_blank'>Visual Object Tagging Tool (VoTT)</a>.\n\nIn this example, the images have already been processed and the coordinates of bounding boxes for the apple and carrot images they contain are stored in text files. Run the following cell to view the first few rows from each file."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\ndf=pd.read_csv('data/apples.txt', sep=',',header=None)\nprint('Apples:')\nprint(df.head())\ndf=pd.read_csv('data/carrots.txt', sep=',',header=None)\nprint('Carrots:')\nprint(df.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The text files contain the following information for each tagged object:\n* The file name\n* The x pixel coordinate of the top-left corner\n* The y pixel coordinate of the top-left corner\n* The x pixel coordinate of the bottom-right corner\n* The y pixel coordinate of the bottom-right corner\n\n## Prepare the training data\nDepending on the framework you are using to train the model, you will need to convert the image bounding box data into the approriate format. In this case, we're using the Custom Vision service; which needs the following data for each object in the training files:\n* The tag ID that defines the class of the object.\n* The normalized left pixel location\n* The normalized top pixel location\n* The normalized width of the object\n* The normalized height of the object\n\n> The *normalized* pixel values are calculated as the proportion of the actual image size between 0 and 1.\n\nThe following cell defines a function to calculate the normalized bounding box coordinates and dimensions for each objects in a text file, and add them to a list of all objects. This function is then called for the apple and carrot files to define the complete collection of tagged objects."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def tag_images(txtFile, tag):\n    import pandas as pd\n    import numpy as np\n    from matplotlib import image as mpimg\n\n    print (\"Tagging images from\", txtFile)\n    df=pd.read_csv(txtFile, sep=',',header=None)\n    for row in df.values:\n        file_name,l,t,r,b = row\n        print(file_name)\n        #normalize values\n        img = mpimg.imread(file_name)\n        img_h, img_w, img_ch = img.shape\n        l = l / img_w\n        r = r / img_w\n        t = t/img_h\n        b = b / img_h\n\n        w = r-l\n        h = b-t\n\n        regions = [Region(tag_id=tag, left=l,top=t,width=w,height=h) ]\n\n        with open(file_name, mode=\"rb\") as image_contents:\n            tagged_images_with_regions.append(ImageFileCreateEntry(name=file_name, contents=image_contents.read(), regions=regions))\n\n\ntagged_images_with_regions = []\ntag_images('data/apples.txt', apple_tag.id)\ntag_images('data/carrots.txt', carrot_tag.id)\nprint('Adding images to project...')\ntrainer.create_images_from_files(project.id, images=tagged_images_with_regions)\nprint(\"Images added!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train the project\nNow that we have the object class and location information, we can train the project and create a predictive model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\n\nprint (\"Training...\")\niteration = trainer.train_project(project.id)\nwhile (iteration.status != \"Completed\"):\n    iteration = trainer.get_iteration(project.id, iteration.id)\n    print (\"Training status: \" + iteration.status)\n    time.sleep(1)\n\n# The iteration is now trained. Publish it to the project endpoint\ntrainer.publish_iteration(project.id, iteration.id, \"First Iteration\", PREDICTION_RESOURCE_ID)\n\n# Make it the default iteration\niteration = trainer.update_iteration(project_id= project.id, iteration_id=iteration.id, name= \"First Iteration\", is_default=True)\n\nprint (\"Trained!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Detect objects in a new image\nNow that you have a trained model, you can use it to find objects in new images."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\nfrom matplotlib import image as mpimg\nfrom matplotlib import pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nimport numpy as np\n%matplotlib inline\n\n# Now there is a trained endpoint that can be used to make a prediction\ntest_img_file = \"data/produce_test.jpg\"\ntest_img = Image.open(test_img_file)\ntest_img_h, test_img_w, test_img_ch = np.array(test_img).shape\n    \npredictor = CustomVisionPredictionClient(PREDICTION_KEY, endpoint=ENDPOINT)\n\n# Open the sample image and get back the prediction results.\nwith open(test_img_file, mode=\"rb\") as test_data:\n    results = predictor.detect_image(project.id, iteration.name, test_data)\n\n\n# Display the results.\ndraw = ImageDraw.Draw(test_img)\n\nfor prediction in results.predictions:\n    if (prediction.probability*100) > 50:\n        print (\"\\t\" + prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100))  \n        left = prediction.bounding_box.left * test_img_w \n        top = prediction.bounding_box.top * test_img_h \n        height = prediction.bounding_box.height * test_img_h\n        width =  prediction.bounding_box.width * test_img_w\n        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\n        draw.line(points, fill='magenta', width=20)\n        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top-20))\n        \nplt.imshow(test_img)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Learn More:\n<a href='https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial-od' target='_blank'>Building an Object Detection solution with the Custom Vision service</a>"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}