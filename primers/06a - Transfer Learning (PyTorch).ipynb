{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Transfer Learning\nA Convolutional Neural Network (CNN) for image classification is made up of multiple layers that extract features, such as edges, corners, etc; and then use a final fully-connected layer to classify objects based on these features. You can visualize this like this:\n\n<table>\n    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n    <tr><td colspan=4 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n</table>\n\n*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.\n\nHow does this help? Well, think of it this way. Suppose you take a professional tennis player and a complete beginner, and try to teach them both how to play raquetball. It's reasonable to assume that the professional tennis player will be easier to train, because many of the underlying skills involved in raquetball are already learned. Similarly, a pre-trained CNN model may be easier to train to classify specific set of objects because it's already learned how to identify the features of common objects, such as edges and corners. Fundamentally, a pre-trained model can be a great way to produce an effective classifier even when you have limited data with which to train it.\n\nIn this notebook, we'll see how to implement transfer learning for a classification model using PyTorch.\n\n## Functions to generate some image data\nFirst, we'll generate our image data, which will consist of only 150 images. In reality, we'd use images of real objects; but we'll just generate some images of basic geometric shapes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# function to generate an image of random size and color\ndef create_image (size, shape):\n    from random import randint\n    import numpy as np\n    from PIL import Image, ImageDraw\n    \n    xy1 = randint(10,40)\n    xy2 = randint(60,100)\n    col = (randint(0,200), randint(0,200), randint(0,200))\n\n    img = Image.new(\"RGB\", size, (255, 255, 255))\n    draw = ImageDraw.Draw(img)\n    \n    if shape == 'circle':\n        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n    elif shape == 'triangle':\n        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n    else: # square\n        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n    del draw\n    \n    return np.array(img)\n\n# function to create a dataset of images\ndef generate_image_data (classes, size, cases, img_dir):\n    import os, shutil\n    from PIL import Image\n    \n    if os.path.exists(img_dir):\n        replace_folder = input(\"Image folder already exists. Enter Y to replace it (this can take a while!). \\n\")\n        if replace_folder == \"Y\":\n            print(\"Deleting old images...\")\n            shutil.rmtree(img_dir)\n        else:\n            return # Quit - no need to replace existing images\n    os.makedirs(img_dir)\n    print(\"Generating new images...\")\n    i = 0\n    while(i < (cases - 1) / len(classes)):\n        if (i%25 == 0):\n            print(\"Progress:{:.0%}\".format((i*len(classes))/cases))\n        i += 1\n        for classname in classes:\n            img = Image.fromarray(create_image(size, classname))\n            saveFolder = os.path.join(img_dir,classname)\n            if not os.path.exists(saveFolder):\n                os.makedirs(saveFolder)\n            imgFileName = os.path.join(saveFolder, classname + str(i) + '.jpg')\n            try:\n                img.save(imgFileName)\n            except:\n                try:\n                    # Retry (resource constraints in Azure notebooks can cause occassional disk access errors)\n                    img.save(imgFileName)\n                except:\n                    # We gave it a shot - time to move on with our lives\n                    print(\"Error saving image\", imgFileName)\n            \n# Our classes will be circles, squares, and triangles\nclassnames = ['circle', 'square', 'triangle']\n\n# All images will be 128x128 pixels\nimg_size = (128,128)\n\n# We'll store the images in a folder named 'shapes'\nfolder_name = 'small_shapes'\n\n# Generate 90 random images.\ngenerate_image_data(classnames, img_size, 90, folder_name)\n\nprint(\"Image files ready in %s folder!\" % folder_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Setting up the Frameworks\nNow that we have our data, we're ready to build a classifier from a pre-trained CNN. The first step is to install and configure the frameworks we want to use.\n\n*Note: For instructions on how to install the PyTorch and TorchVision packages on your own system, see https://pytorch.org/get-started/locally/*"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install PyTorch\n!pip install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n!pip install torchvision\n\n# Import PyTorch Libraries\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint('Ready to train a model using PyTorch', torch.__version__)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Downloading a trained model to user as a base\nThe ***resnet*** model is an CNN-based image classifier that has been pre-trained using a huge dataset containing thousands of images of many kinds of object. We'll download the trained model, excluding its final linear layer, and freeze the convolutional layers to retain the trained weights. Then we'll add a new linear layer that will map the features extracted by the convolutional layers to the classes of our shape images."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_resnet = torchvision.models.resnet18(pretrained=True)\nfor param in model_resnet.parameters():\n    param.requires_grad = False\n\nnum_ftrs = model_resnet.fc.in_features\nmodel_resnet.fc = nn.Linear(num_ftrs, len(classnames))\n\n# Now print the full model, which will include the layers of the base model plus the linear layer we added\nprint(model_resnet)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Loading and Preparing the Data\nBefore we can train the model to classify images based on our shape classes, we need to prepare the training data. PyTorch includes functions for loading and transforming data. We'll use these to create an iterative loader for training data, and a second iterative loader for test data (which we'll use to validate the trained model). The loaders will transform the image data to match the format used to train the original resnet CNN model, and finally convert the image data into *tensors*, which are the core data structure used in PyTorch.\n\nRun the following cell to define the data loaders, and then load the first batch of 32 training images and display them along with their class labels."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to ingest data using training and test loaders\ndef load_dataset(data_path):\n    \n    # Resize to 256 x 256, center-crop to 224x224 (to match the resnet image size), and convert to Tensor\n    transformation = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    # Load all of the images, transforming them\n    full_dataset = torchvision.datasets.ImageFolder(\n        root=data_path,\n        transform=transformation\n    )\n    \n    # Split into training (70%) and testing (30%) datasets)\n    train_size = int(0.7 * len(full_dataset))\n    test_size = len(full_dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n    \n    # define a loader for the training data we can iterate through in 32-image batches\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=4,\n        num_workers=0,\n        shuffle=False\n    )\n    \n    # define a loader for the testing data we can iterate through in 32-image batches\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=4,\n        num_workers=0,\n        shuffle=False\n    )\n        \n    return train_loader, test_loader\n\n\n# Now load the images from the shapes folder\nimport os  \ndata_path = 'small_shapes/'\n\n# Get the class names\nclasses = os.listdir(data_path)\nclasses.sort()\nprint(len(classes), 'classes:')\nprint(classes)\n\n# Get the iterative dataloaders for test and training data\ntrain_loader, test_loader = load_dataset(data_path)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Training the Model\nWith the layers of the CNN defined, we're ready to train it using our image data. The weights used in the feature extraction layers from the base resnet model will not be changed by training, only the final linear layer that maps the features to our shape classes will be trained."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train(model, device, train_loader, optimizer, epoch):\n    # Set the model to training mode\n    model.train()\n    train_loss = 0\n    print(\"Epoch:\", epoch)\n    # Process the images in batches\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Use the CPU or GPU as appropriate\n        data, target = data.to(device), target.to(device)\n        # Reset the optimizer\n        optimizer.zero_grad()\n        # Push the data forward through the model layers\n        output = model(data)\n        # Get the loss\n        loss = loss_criteria(output, target)\n        # Keep a running total\n        train_loss += loss.item()\n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        # Print metrics for every 10 batches so we see some progress\n        if batch_idx % 10 == 0:\n            print('Training set [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n                batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n    # return average loss for the epoch\n    return train_loss / len(train_loader.dataset)\n            \n            \ndef test(model, device, test_loader):\n    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            # Get the predicted classes for this batch\n            output = model(data)\n            # calculate the loss and successful predictions for this batch\n            test_loss += loss_criteria(output, target).item()\n            pred = output.max(1, keepdim=True)[1] \n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    # Calculate the average loss and total accuracy for this epoch\n    test_loss /= len(test_loader.dataset)\n    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    \n    # return average loss for the epoch\n    return test_loss\n    \n    \n# Now use the train and test functions to train and test the model    \n\ndevice = \"cpu\"\nif (torch.cuda.is_available()):\n    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n    device = \"cuda\"\nprint('Training on', device)\n\n# Create an instance of the model class and allocate it to the device\nmodel_resnet = model_resnet.to(device)\n\n# Use an \"Adam\" optimizer to adjust weights\n# (see https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\noptimizer = optim.Adam(model_resnet.parameters(), lr=0.001)\n\n# Specify the loss criteria\nloss_criteria = nn.CrossEntropyLoss()\n\n# Track metrics in these arrays\nepoch_nums = []\ntraining_loss = []\nvalidation_loss = []\n\n# Train over 5 epochs (in a real scenario, you'd likely use many more)\nepochs = 5\nfor epoch in range(1, epochs + 1):\n        train_loss = train(model_resnet, device, train_loader, optimizer, epoch)\n        test_loss = test(model_resnet, device, test_loader)\n        epoch_nums.append(epoch)\n        training_loss.append(train_loss)\n        validation_loss.append(test_loss)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### View the Loss History\nWe tracked average training and validation loss for each epoch. We can plot these to see where the levels of loss converged, and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Evaluate Model Performance\nWe can see the final accuracy based on the test data, but typically we'll want to explore performance metrics in a little mode depth. Let's plot a confusion matrix to see how well the model is predicting each class."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Pytorch doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Set the model to evaluate mode\nmodel_resnet.eval()\n\n# Get predictions for the test data and convert to numpy arrays for use with SciKit-Learn\nprint(\"Getting predictions from test set...\")\ntruelabels = []\npredictions = []\nfor data, target in test_loader:\n    for label in target.cpu().data.numpy():\n        truelabels.append(label)\n    for prediction in model_resnet.cpu()(data).data.numpy().argmax(1):\n        predictions.append(prediction) \n\n# Plot the confusion matrix\ncm = confusion_matrix(truelabels, predictions)\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel(\"Predicted Shape\")\nplt.ylabel(\"True Shape\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using the Trained Model\nNow that we've trained the model, we can use it to predict the class of an image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to predict the class of an image\ndef predict_image(classifier, image_array):\n    from PIL import Image\n    import numpy as np\n    \n    # Set the classifer model to evaluation mode\n    classifier.eval()\n    \n    # These are the classes our model can predict\n    class_names = ['circle', 'square', 'triangle']\n    \n    # Apply the same transformations as we did for the training images\n    transformation = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor()\n    ])\n\n    # Preprocess the imagees\n    image_tensor = torch.stack([transformation(Image.fromarray(image)).float() for image in image_array])\n\n    # Turn the input into a Variable\n    input_features = Variable(image_tensor)\n\n    # Predict the class of each input image (using the CPU/GPU as available)\n    predictions = classifier(input_features.to(device))\n    \n    predicted_classes = []\n    # Convert the predictions to a numpy array on the CPU\n    for prediction in predictions.cpu().data.numpy():\n        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n        # So get the index of the highest probability\n        class_idx = np.argmax(prediction)\n        # Add add the corresponding class name to the results\n        predicted_classes.append(class_names[class_idx])\n    return np.array(predicted_classes)\n\n\n# Now let's try it with a new image\nfrom random import randint\n\n# Create a random test image\nimg = create_image ((224,224), classes[randint(0, len(classes)-1)])\nplt.imshow(img)\n\n# Create an array of (1) images to match the expected input format\nimage_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n\npredicted_classes = predict_image(model_resnet, image_array)\nprint(predicted_classes[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Learning More\n* [PyTorch Documentation](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}