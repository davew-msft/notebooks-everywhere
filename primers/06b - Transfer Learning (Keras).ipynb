{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Transfer Learning\nA Convolutional Neural Network (CNN) for image classification is made up of multiple layers that extract features, such as edges, corners, etc; and then use a final fully-connected layer to classify objects based on these features. You can visualize this like this:\n\n<table>\n    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n    <tr><td colspan=4 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n</table>\n\n*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.\n\nHow does this help? Well, think of it this way. Suppose you take a professional tennis player and a complete beginner, and try to teach them both how to play raquetball. It's reasonable to assume that the professional tennis player will be easier to train, because many of the underlying skills involved in raquetball are already learned. Similarly, a pre-trained CNN model may be easier to train to classify specific set of objects because it's already learned how to identify the features of common objects, such as edges and corners.\n\nIn this notebook, we'll see how to implement transfer learning for a classification model.\n\n## Functions to generate some image data\nFirst, we'll create some functions to generate our image data. In reality, we'd use images of real objects; but we'll just generate a small number of images of basic geometric shapes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# function to generate an image of random size and color\ndef create_image (size, shape):\n    from random import randint\n    import numpy as np\n    from PIL import Image, ImageDraw\n    \n    xy1 = randint(10,40)\n    xy2 = randint(60,100)\n    col = (randint(0,200), randint(0,200), randint(0,200))\n\n    img = Image.new(\"RGB\", size, (255, 255, 255))\n    draw = ImageDraw.Draw(img)\n    \n    if shape == 'circle':\n        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n    elif shape == 'triangle':\n        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n    else: # square\n        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n    del draw\n    \n    return np.array(img)\n\n# function to create a dataset of images\ndef generate_image_data (classes, size, cases, img_dir):\n    import os, shutil\n    from PIL import Image\n    \n    if os.path.exists(img_dir):\n        replace_folder = input(\"Image folder already exists. Enter Y to replace it (this can take a while!). \\n\")\n        if replace_folder == \"Y\":\n            print(\"Deleting old images...\")\n            shutil.rmtree(img_dir)\n        else:\n            return # Quit - no need to replace existing images\n    os.makedirs(img_dir)\n    print(\"Generating new images...\")\n    i = 0\n    while(i < (cases - 1) / len(classes)):\n        if (i%25 == 0):\n            print(\"Progress:{:.0%}\".format((i*len(classes))/cases))\n        i += 1\n        for classname in classes:\n            img = Image.fromarray(create_image(size, classname))\n            saveFolder = os.path.join(img_dir,classname)\n            if not os.path.exists(saveFolder):\n                os.makedirs(saveFolder)\n            imgFileName = os.path.join(saveFolder, classname + str(i) + '.jpg')\n            try:\n                img.save(imgFileName)\n            except:\n                try:\n                    # Retry (resource constraints in Azure notebooks can cause occassional disk access errors)\n                    img.save(imgFileName)\n                except:\n                    # We gave it a shot - time to move on with our lives\n                    print(\"Error saving image\", imgFileName)\n            \n# Our classes will be circles, squares, and triangles\nclassnames = ['circle', 'square', 'triangle']\n\n# All images will be 128x128 pixels\nimg_size = (128,128)\n\n# We'll store the images in a folder named 'shapes'\nfolder_name = 'small_shapes'\n\n# Generate 90 random images.\ngenerate_image_data(classnames, img_size, 90, folder_name)\n\nprint(\"Image files ready in %s folder!\" % folder_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Setting up the Frameworks\nNow that we have our data, we're ready to build a CNN. The first step is to import and configure the framework we want to use."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade keras\n\nimport keras\nfrom keras import backend as K\n\nprint('Keras version:',keras.__version__)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Preparing the Data\nBefore we can train the model, we need to prepare the data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.image import ImageDataGenerator\n\ndata_folder = 'small_shapes'\n# Our source images are 128x128, but the base model we're going to use was trained with 224x224 images\npretrained_size = (224,224)\nbatch_size = 15\n\nprint(\"Getting Data...\")\ndatagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n                             validation_split=0.3) # hold back 30% of the images for validation\n\nprint(\"Preparing training dataset...\")\ntrain_generator = datagen.flow_from_directory(\n    data_folder,\n    target_size=pretrained_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training') # set as training data\n\nprint(\"Preparing validation dataset...\")\nvalidation_generator = datagen.flow_from_directory(\n    data_folder,\n    target_size=pretrained_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation') # set as validation data\n\nclassnames = list(train_generator.class_indices.keys())\nprint(\"class names: \", classnames)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Downloading a trained model to use as a base\nThe VGG16 model is an image classifier that was trained on the ImageNet dataset - a huge dataset containing thousands of images of many kinds of object. We'll download the trained model, excluding its top layer, and set its input shape to match our image data.\n\n*Note: The **keras.applications** namespace includes multiple base models, some which may perform better for your dataset than others. I've chosen this model because it's fairly lightweight within the limited resources of the Azure Notebooks environment.*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras import applications\n#Load the base model, not including its final connected layer, and set the input shape to match our images\nbase_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=train_generator.image_shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Freeze the already trained layers and add a custom output layer for our classes\nThe existing feature extraction layers are already trained, so we just need to add a couple of layers so that the model output is the predictions for our classes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras import Model\nfrom keras.layers import Flatten, Dense\n\n# Freeze the already-trained layers in the base model\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Create layers for classification of our images\nx = base_model.output\nx = Flatten()(x)\nprediction_layer = Dense(len(classnames), activation='sigmoid')(x) \nmodel = Model(inputs=base_model.input, outputs=prediction_layer)\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# Now print the full model, which will include the layers of the base model plus the dense layer we added\nprint(model.summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Training the Model\nWith the layers of the CNN defined, we're ready to train the top layer using our image data. This will take a considerable amount of time on a CPU depending on the complexity of the base model."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train the model over 2 epochs using 15-image batches and using the validation holdout dataset for validation\nnum_epochs = 2\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = num_epochs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### View the Loss History\nWe tracked average training and validation loss for each epoch. We can plot these to see where the levels of loss converged, and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nepoch_nums = range(1,num_epochs+1)\ntraining_loss = history.history[\"loss\"]\nvalidation_loss = history.history[\"val_loss\"]\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using the Trained Model\nNow that we've trained the model, we can use it to predict the class of an image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def predict_image(classifier, image_array):\n    import numpy as np\n    \n    # We need to format the input to match the training data\n    # The generator loaded the values as floating point numbers\n    # and normalized the pixel values, so...\n    imgfeatures = image_array.astype('float32')\n    imgfeatures /= 255\n    \n    # These are the classes our model can predict\n    classes = ['circle', 'square', 'triangle']\n    \n    # Predict the class of each input image\n    predictions = classifier.predict(imgfeatures)\n    \n    predicted_classes = []\n    for prediction in predictions:\n        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n        # So get the index of the highest probability\n        class_idx = np.argmax(prediction)\n        # And append the corresponding class name to the results\n        predicted_classes.append(classes[int(class_idx)])\n    # Return the predictions as a JSON\n    return predicted_classes\n\n\nfrom random import randint\nfrom PIL import Image\nimport numpy as np\n%matplotlib inline\n\n# Create a random test image\nimg = create_image ((224,224), classnames[randint(0, len(classnames)-1)])\nplt.imshow(img)\n\n# Create an array of (1) images to match the expected input format\nimg_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n\n# get the predicted clases\npredicted_classes = predict_image(model, img_array)\n\n# Display the prediction for the first image (we only submitted one!)\nprint(predicted_classes[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Learning More\n* [Tensorflow Documentation](https://www.tensorflow.org/get_started/premade_estimators)\n* [Keras Documentation](https://keras.io/)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}