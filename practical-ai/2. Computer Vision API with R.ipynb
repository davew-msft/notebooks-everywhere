{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Computer Vision API\n\nThis script explores using the Cognitive Services Vision API via its REST API. \n\nHere, we'll write an R function to extract a random\nimage from Wikimedia Commons, and another function to generate a caption of the image using the Vision API. You can see the end result in this [blog post](http://blog.revolutionanalytics.com/2018/03/computer-vision-api.html).\n\nThe concepts are mostly explained as we go, but if you\nwant to find more information, take a look here:\n\n* [Computer Vision Overview](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/?WT.mc_id=AILive-workshop-davidsmi)\n* [Computer Vision API 1.0 Reference](https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa)\n\n## Using this Notebook\n\nThe scripts \nare provided as Jupyter Notebooks within the [Azure Notebooks](https://notebooks.azure.com/?WT.mc_id=ODSC-workshop-davidsmi) service.\nYou don't need a Microsoft\nAccount to view the scripts, but you will need to set one up and generate keys in Azure\nto run\nthe examples. All of the examples use free Azure services.\n\nIf you're new to Jupyter Notebooks, here's a quick intro:\n\n1. Click within a cell, and then press `Ctrl+Enter` to run (or render) the current cell.\n2. You'll see a number to the left of the cell when the computations are complete, like this: `In [1]:`. (The number represents the order of computations.) If there's output, it will print below the cell. You may have to scroll up to see it all.\n3. Run each cell, in order, from to bottom.\n4. To download/upload files, return to the Project view (click \"Practical AI Workshop\" in the top-right), and then use the functions in the toolbar on that page.\n\nFor more information about Notebooks, check out the [Jupyter Notebook documentation](http://jupyter.readthedocs.io/en/latest/index.html).\n\nIf you're new to R, you might want to start with this [Introduction to\nR](https://notebooks.azure.com/davidsmi/libraries/intro-r?WT.mc_id=ODSC-workshop-davidsmi) notebook\nto get a sense of the language.\n\n## First, clone these workshop materials\n\n1. Visit https://notebooks.azure.com/davidsmi/projects/PracticalAI\n\n    * Sign in with your Microsoft account if needed  \n\n1. click Clone in the toolbar, to create a copy of the workshop materials in your own Azure Notebooks library.\n\n\n## Connecting to Azure services\n\nYou will need:\n\n1. A [Microsoft account](https://account.microsoft.com/account). You can use an existing Outlook 365\nor Xbox Live account, or create a new one.\n\n1. A Microsoft Azure subscription. If you don't already have an Azure subscription, you can visit\n[https://cda.ms/kT](https://cda.ms/kT) and also get \\$200 in credits to use with paid services. You'll need to provide\na credit or debit card, but everything we'll be doing is free to use. If you're a student, you can \nalso register at [https://cda.ms/kY](https://cda.ms/kY) without a credit card for a \\$100 credit.\n\nYou'll also need a few other things specific to this workshop. Follow the instructions below to \nset up everything you need.\n\n## Log in to the Azure Portal\n\n1. Visit https://portal.azure.com \n2. Sign in with your Microsoft Account. If you don't have a Microsoft account, use the \n   links above to create one for free.\n\n## Create an Azure Resource Group\n\nIn Azure, a Resource Group is a collection of services you have created. It groups services\ntogether, and makes it easy to bulk-delete things later on. We'll create one for this lab.\n\n1. Visit https://portal.azure.com (and sign in if needed)\n2. Click \"Resource Groups\" in the left column\n3. Click \"+ Add\"\n    * Resource Group Name: ailive\n    * Subscription: _there should be just one option_\n    * Resource Group Location: South Central US\n4. Click \"Create\"\n   \nA notification will appear in the top right. Click the button \"Pin to Dashboard\" to pin this resource group to your home page in the Azure portal, as you'll be referring to it frequently.\n\n## Create authorization keys for Computer Vision\n\n1. Visit https://portal.azure.com (and sign in if needed)\n2. Click \"+ Create a Resource\" (top-left corner)\n3. Click \"AI + Machine Learning\"\n4. Click \"Computer Vision\"\n    * Name: ailive-vision\n    * Subscription: _there should be just one option_\n    * Location: South Central US\n    * Pricing Tier: F0 (free, 20 calls per minute)\n    * Resource Group: Use existing \"ailive\" group\n5. Click \"Create\"\n\nAfter a few moments you will get a message that your keys have been generated, after which you can move to the next section.\n\n\n\nOnce you've done this for all the cognitive services, save the file `keys.txt` and upload it to \nreplace the existing file i## Modify the keys.txt file\n\nEdit the `keys.txt` file to provide the necessary keys. In Azure Notebooks, you select the file and press `i` to edit it directly. (Alternatively, you can download the file `keys.txt` -- highlight it in the Library view and then press `d` or click the download icon in the toolbar -- and edit it with an editor, then upload the modified file.)\n\nFor the first line\nof the file, `region`, change the value to `southcentralus`. \n\nFor the second line of the file, `vision,` visit your `ailive` resource\ngroup in the [Azure Portal](https://portal.azure.com?WT.mc_id=AILive-workshop-davidsmi) and then:\n\n1. Click on the API resource for Computer Vision `ailive-vision`\n2. In the menu, click on \"keys\"\n3. Click the \"copy to clipboard\" next to KEY 1. (You can ignore KEY 2).\n4. Paste the key into the `vision` entry in keys.txt\n\nYou can ignore the remaining lines of `keys.txt` for now.\n\nYour final `keys.txt` file will look like this, but with different (working) keys:\n\n```\n       key\nregion southcentralus\nvision 7f1f01ac24064abd80970f41a90237e7\ncustom 1632b49e2930430694a9bbd3ab0c0cc2\ncvpred 37eb1f0e5fd34253939350197ae3d933\n```\n\nNow you can run the R code below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## load some packages required by the code below. \n## These packages come pre-installed in the Azure Notebook service,\n## but if you try this code elsewhere you may need to install them first with install.packages\nlibrary(tools)\nlibrary(httr)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "               _                           \nplatform       x86_64-pc-linux-gnu         \narch           x86_64                      \nos             linux-gnu                   \nsystem         x86_64, linux-gnu           \nstatus                                     \nmajor          3                           \nminor          4.1                         \nyear           2017                        \nmonth          06                          \nday            30                          \nsvn rev        72865                       \nlanguage       R                           \nversion.string R version 3.4.1 (2017-06-30)\nnickname       Single Candle               "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Retrieve API keys and region from keys.txt file. \n## See above for how to obtain the necessary keys and modify the file accordingly.\n\nkeys <- read.table(\"keys.txt\", header=TRUE, stringsAsFactors = FALSE)\nvision_api_key <- keys[\"vision\",1]\nazure_region <- keys[\"region\",1]\nvision_api_endpoint <- paste0(\"https://\", azure_region, \".api.cognitive.microsoft.com/vision/v1.0\")\ncat(\"The region is:\",azure_region,\"\\n\")\n\n## If you see ERROR-EDIT-KEYS.txt-FILE here, you need to edit keys.txt as described above",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "The region is: southcentralus \n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Here are some URLs of example images you can try out later.\n## Feel free to find other images you want to use.\n## I visited https://en.wikipedia.org/wiki/Special:Random to go to a random Wikipedia page\n## and downloaded images from there. The Large size works with API limits\n\nexample_images =c(\n ## animals\n 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Pair_of_Merops_apiaster_feeding.jpg/1200px-Pair_of_Merops_apiaster_feeding.jpg',\n 'https://upload.wikimedia.org/wikipedia/commons/4/4f/Queenie.JPG', \n 'https://upload.wikimedia.org/wikipedia/commons/3/34/Ectopsocus_briggsi.jpg',\n ## buildings, workplaces\n 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Prze%C5%82%C4%99cz_Okraj-przejscie_graniczne.jpg/1200px-Prze%C5%82%C4%99cz_Okraj-przejscie_graniczne.jpg',\n 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/61/Wasseiges_JPG04.jpg/1200px-Wasseiges_JPG04.jpg',\n 'https://upload.wikimedia.org/wikipedia/commons/5/58/St_george_edgbaston.jpg',\n 'https://upload.wikimedia.org/wikipedia/commons/0/02/Atlanta_College_of_Art_Print_Making_Studio.jpg',\n ## non-photos \n 'https://upload.wikimedia.org/wikipedia/commons/1/15/M15_%28Ukraine%29.png',\n ## people, faces\n 'https://upload.wikimedia.org/wikipedia/en/1/1b/I_Remember_You_%28John_Hicks_album%29.jpg',\n 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/FIS_Ski_Jumping_World_Cup_2014_-_Engelberg_-_20141221_-_Shohei_Tochimoto.jpg/1200px-FIS_Ski_Jumping_World_Cup_2014_-_Engelberg_-_20141221_-_Shohei_Tochimoto.jpg',\n 'https://upload.wikimedia.org/wikipedia/en/d/d7/Grover_Washabaugh.jpg',\n ## things that make the API throw errors\n 'https://upload.wikimedia.org/wikipedia/commons/3/3a/FIS_Ski_Jumping_World_Cup_2014_-_Engelberg_-_20141221_-_Shohei_Tochimoto.jpg', # too large\n 'error' #malformed URL\n )\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## In this section, we'll call the Computer Vision API manually\n## Later, we'll write a function to automate the process\n\n#image_url =\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ef4d0bc7-2c45-4d17-afb1-9cad8f293657.jpg\"\nimage_url = example_images[3] \n## feel free to try a different example, or provide a URL of your own choice\n\nvisualFeatures = \"Description,Tags,Categories,Faces\"\n# choose the image information to return\n# options = \"Categories, Tags, Description, Faces, ImageType, Color, Adult\"\n\ndetails = \"Landmarks,Celebrities\"\n# Ask the Computer Vision API to detect names of celebrities or famous landmarks\n\nreqURL = paste0(vision_api_endpoint,\n               \"/analyze?visualFeatures=\",\n               visualFeatures,\n               \"&details=\",\n               details)\n\nprint(reqURL)\n\nAPIresponse = POST(url = reqURL,\n                   content_type('application/json'),\n                   add_headers(.headers = c('Ocp-Apim-Subscription-Key' = vision_api_key)),\n                   body=list(url = image_url),\n                   encode = \"json\") \n\ndf = content(APIresponse)\n\n## display caption and confidence\ncat(image_url,\"\\n\")\ndf$description$captions[[1]]$text\ndf$description$captions[[1]]$confidence",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[1] \"https://southcentralus.api.cognitive.microsoft.com/vision/v1.0/analyze?visualFeatures=Description,Tags,Categories,Faces&details=Landmarks,Celebrities\"\nhttps://upload.wikimedia.org/wikipedia/commons/3/34/Ectopsocus_briggsi.jpg \n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "[1] \"a insect on the ground\"",
            "text/latex": "'a insect on the ground'",
            "text/markdown": "'a insect on the ground'",
            "text/html": "'a insect on the ground'"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "[1] 0.8604853",
            "text/latex": "0.860485297512044",
            "text/markdown": "0.860485297512044",
            "text/html": "0.860485297512044"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Explore that `df` object to see what other information is returned by the API (try: `print(df)`). We'll just be looking at the\ngenerated image caption for now."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(df)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "$categories\n$categories[[1]]\n$categories[[1]]$name\n[1] \"abstract_\"\n\n$categories[[1]]$score\n[1] 0.00390625\n\n\n$categories[[2]]\n$categories[[2]]$name\n[1] \"others_\"\n\n$categories[[2]]$score\n[1] 0.00390625\n\n\n\n$tags\n$tags[[1]]\n$tags[[1]]$name\n[1] \"insect\"\n\n$tags[[1]]$confidence\n[1] 0.9918659\n\n\n$tags[[2]]\n$tags[[2]]$name\n[1] \"animal\"\n\n$tags[[2]]$confidence\n[1] 0.9892629\n\n\n$tags[[3]]\n$tags[[3]]$name\n[1] \"half\"\n\n$tags[[3]]$confidence\n[1] 0.4304085\n\n\n$tags[[4]]\n$tags[[4]]$name\n[1] \"fly\"\n\n$tags[[4]]$confidence\n[1] 0.4304085\n\n\n$tags[[5]]\n$tags[[5]]$name\n[1] \"wasp\"\n\n$tags[[5]]$confidence\n[1] 0.1212679\n\n\n$tags[[6]]\n$tags[[6]]$name\n[1] \"lepidoptera\"\n\n$tags[[6]]$confidence\n[1] 0.04301453\n\n\n\n$description\n$description$tags\n$description$tags[[1]]\n[1] \"insect\"\n\n$description$tags[[2]]\n[1] \"animal\"\n\n$description$tags[[3]]\n[1] \"piece\"\n\n$description$tags[[4]]\n[1] \"table\"\n\n$description$tags[[5]]\n[1] \"food\"\n\n$description$tags[[6]]\n[1] \"sitting\"\n\n$description$tags[[7]]\n[1] \"board\"\n\n$description$tags[[8]]\n[1] \"half\"\n\n$description$tags[[9]]\n[1] \"small\"\n\n$description$tags[[10]]\n[1] \"brown\"\n\n$description$tags[[11]]\n[1] \"beach\"\n\n$description$tags[[12]]\n[1] \"holding\"\n\n$description$tags[[13]]\n[1] \"water\"\n\n$description$tags[[14]]\n[1] \"apple\"\n\n$description$tags[[15]]\n[1] \"laying\"\n\n$description$tags[[16]]\n[1] \"wooden\"\n\n$description$tags[[17]]\n[1] \"dog\"\n\n$description$tags[[18]]\n[1] \"standing\"\n\n$description$tags[[19]]\n[1] \"fruit\"\n\n$description$tags[[20]]\n[1] \"sand\"\n\n$description$tags[[21]]\n[1] \"surfing\"\n\n$description$tags[[22]]\n[1] \"cutting\"\n\n$description$tags[[23]]\n[1] \"man\"\n\n\n$description$captions\n$description$captions[[1]]\n$description$captions[[1]]$text\n[1] \"a insect on the ground\"\n\n$description$captions[[1]]$confidence\n[1] 0.8604853\n\n\n\n\n$faces\nlist()\n\n$requestId\n[1] \"f4065fa0-3854-435e-ae74-8bd1e761ffde\"\n\n$metadata\n$metadata$width\n[1] 1255\n\n$metadata$height\n[1] 831\n\n$metadata$format\n[1] \"Jpeg\"\n\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's define a function in R to apply the Computer Vision API to an image in a URL, and print out the image caption returned by the API."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_caption <- function(URL) {\n reqURL = paste0(vision_api_endpoint,\n                 \"/analyze?visualFeatures=Description\",\n                 \"&details=Celebrities,Landmarks\")\n \n APIresponse = POST(url = reqURL,\n                    content_type('application/json'),\n                    add_headers(.headers = c('Ocp-Apim-Subscription-Key' = vision_api_key)),\n                    body=list(url = URL),\n                    encode = \"json\") \n \n df = content(APIresponse)\n cat(URL, \"\\n\")\n\n  ## when we get Wikimedia Commons images later, we'll grab their description too, and display it if so\n if(!is.null(attr(URL,\"desc\"))) \n  cat(\"Wikimedia Commons description:\\n\", attr(URL,\"desc\"),  \"\\n\")\n\n cat(\"Vision API description:\\n\",  df$description$captions[[1]]$text,\"\\n\")\n cat(paste0(\"Confidence: \",df$description$captions[[1]]$confidence,\"\\n\"))\n invisible(df)\n}",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try it out:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_caption(\"http://media.timeout.com/images/100004257/630/472/image.jpg\")",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "http://media.timeout.com/images/100004257/630/472/image.jpg \nVision API description:\n a large white building with Sacré-Cœur, Paris in the background \nConfidence: 0.851741857095374\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try some more images. We can write a function to return the URL of a random image in Wikimedia Commons, which will\ngive us unlimited images to work with. We'll also check that the image meets the Computer Vision API restrictions \n(minimum dimensions 50x50, maximum file size 4Mb, certain image formats)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "random_image <- function() {\n ## Return the URL of a random image in Wikimedia Commons\n random_query <- paste0(\"https://commons.wikimedia.org/w/api.php?\",\n                        \"action=query\",\n                        \"&generator=random\", # get a random page\n                        \"&grnlimit=1\",       # return 1 page\n                        \"&grnnamespace=6\",   # category: File\n                        \"&prop=imageinfo\",\n                        \"&iiprop=url|size|extmetadata\",\n                        \"&iiurlheight=1080\",  # limit images height (sometimes)\n                        \"&format=json&formatversion=2\")\n random_response <- POST(random_query)\n output <- content(random_response)\n url <- output$query$pages[[1]]$imageinfo[[1]]$url\n\n ## check the image metadata, and throw an error if it won't work with the \n ## Computer Vision API\n ext <- tolower(file_ext(url))\n w <- output$query$pages[[1]]$imageinfo[[1]]$width\n h <- output$query$pages[[1]]$imageinfo[[1]]$height\n size <- output$query$pages[[1]]$imageinfo[[1]]$size\n desc <- output$query$pages[[1]]$imageinfo[[1]]$extmetadata$ImageDescription$value \n if(w<50 || h<50) stop(\"Image too small\") \n if(size > 4000000) stop(\"Image too large\")\n if(!(ext %in% c(\"jpg\",\"jpeg\",\"png\",\"gif\",\"bmp\"))) stop(paste(\"invalid image type:\",ext))\n\n ## In addition to the URL, return the dimensions and Wikimedia description as attributes\n attr(url, \"dims\") <- c(w=w,h=h)\n attr(url, \"desc\") <- desc\n url\n} ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "u <- random_image()\nimage_caption(u)\n\n# You might see an \"image too large\" or other error; if that happens just run this chunk again to try a different image\n# In some instances you may get no output from the Vision API. This is likely caused by an image of the wrong format or size.",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "https://upload.wikimedia.org/wikipedia/commons/3/3a/Wire_bush_%281391111745%29.jpg \nWikimedia Commons description:\n <p><a rel=\"nofollow\" class=\"external text\" href=\"https://www.flickr.com/search/?q=Sterculiaceae&amp;w=91314344@N00\">Sterculiaceae</a> (cacao family) » <i><a rel=\"nofollow\" class=\"external text\" href=\"https://www.flickr.com/search/?q=Melochiacorchorifolia&amp;w=91314344@N00&amp;m=tags\">Melochia corchorifolia</a></i>\n</p>\n<p><b>commonly known as</b>: chocolateweed, wire bush • <b>Hindi</b>: bundahia, bundava, चित्रबीज chitrabeez, thulak, tikiokra • <b>Malayalam</b>: niruren, tsjeru-uren • <b>Marathi</b>: लहान मेथुरी lahan methuri • <b>Tamil</b>: பிண்ணாக்குக்கீரை pinnakkukkirai • <b>Telugu</b>: gangupindi, kura, sitha kura, sitnata kura, tutturubenda\n</p>\n<p>... herb or undershrub ... 1.5-2 ft. or more, with hollow stems ... erect or sometimes prostrate.\n</p>\n<p>... small flowers, white or sometimes yellowish or pinkish ... in terminal clusters.\n</p>\n<p><b>References</b>: <i>Further Flowers of Sahyadri</i> by Shrikant Ingalhalikar • <a rel=\"nofollow\" class=\"external text\" href=\"http://www.hear.org/pier/species/melochia_corchorifolia.htm\">PIER species info</a> • <a rel=\"nofollow\" class=\"external text\" href=\"http://forest.ap.nic.in/Forest%20Flora%20of%20Andhra%20Pradesh/files/ff0203.htm\">Forest Flora of Andhra Pradesh</a>\n</p> \nVision API description:\n a close up of a green plant \nConfidence: 0.9609719419799\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "r",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}